{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train_preprocessed.csv\")\n",
    "train_x = train.drop([\"target\"], axis=1)\n",
    "train_y = train[\"target\"]\n",
    "\n",
    "test_x = pd.read_csv(\"../data/test_preprocessed.csv\")\n",
    "test_x = test_x.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>product</th>\n",
       "      <th>amount</th>\n",
       "      <th>medical_info_a1</th>\n",
       "      <th>medical_info_a2</th>\n",
       "      <th>medical_info_a3</th>\n",
       "      <th>medical_info_b1</th>\n",
       "      <th>...</th>\n",
       "      <th>medical_keyword_6</th>\n",
       "      <th>medical_keyword_7</th>\n",
       "      <th>medical_keyword_8</th>\n",
       "      <th>medical_keyword_9</th>\n",
       "      <th>medical_keyword_10</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>yearmonth</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>166.445608</td>\n",
       "      <td>65.016732</td>\n",
       "      <td>9</td>\n",
       "      <td>7000000</td>\n",
       "      <td>134</td>\n",
       "      <td>202</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>24182</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>164.334615</td>\n",
       "      <td>56.544217</td>\n",
       "      <td>0</td>\n",
       "      <td>7000000</td>\n",
       "      <td>438</td>\n",
       "      <td>263</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>24185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>167.462917</td>\n",
       "      <td>54.242267</td>\n",
       "      <td>2</td>\n",
       "      <td>6000000</td>\n",
       "      <td>313</td>\n",
       "      <td>325</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>24194</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>177.097725</td>\n",
       "      <td>71.147762</td>\n",
       "      <td>3</td>\n",
       "      <td>8000000</td>\n",
       "      <td>342</td>\n",
       "      <td>213</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>24187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>158.165788</td>\n",
       "      <td>65.240697</td>\n",
       "      <td>1</td>\n",
       "      <td>9000000</td>\n",
       "      <td>327</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>24201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>182.729800</td>\n",
       "      <td>73.393777</td>\n",
       "      <td>1</td>\n",
       "      <td>2000000</td>\n",
       "      <td>189</td>\n",
       "      <td>232</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>24190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>167.701136</td>\n",
       "      <td>75.006529</td>\n",
       "      <td>8</td>\n",
       "      <td>9000</td>\n",
       "      <td>426</td>\n",
       "      <td>202</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>24185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>145.609998</td>\n",
       "      <td>47.739397</td>\n",
       "      <td>8</td>\n",
       "      <td>1000</td>\n",
       "      <td>370</td>\n",
       "      <td>274</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>24194</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>165.796017</td>\n",
       "      <td>57.567695</td>\n",
       "      <td>6</td>\n",
       "      <td>5000</td>\n",
       "      <td>291</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>24194</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>180.301762</td>\n",
       "      <td>71.425135</td>\n",
       "      <td>4</td>\n",
       "      <td>1000000</td>\n",
       "      <td>288</td>\n",
       "      <td>454</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>24187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex      height     weight  product   amount  medical_info_a1  \\\n",
       "0      50    1  166.445608  65.016732        9  7000000              134   \n",
       "1      68    0  164.334615  56.544217        0  7000000              438   \n",
       "2      77    1  167.462917  54.242267        2  6000000              313   \n",
       "3      17    1  177.097725  71.147762        3  8000000              342   \n",
       "4      62    0  158.165788  65.240697        1  9000000              327   \n",
       "...   ...  ...         ...        ...      ...      ...              ...   \n",
       "9995   61    1  182.729800  73.393777        1  2000000              189   \n",
       "9996   33    0  167.701136  75.006529        8     9000              426   \n",
       "9997   44    0  145.609998  47.739397        8     1000              370   \n",
       "9998   34    0  165.796017  57.567695        6     5000              291   \n",
       "9999   31    1  180.301762  71.425135        4  1000000              288   \n",
       "\n",
       "      medical_info_a2  medical_info_a3  medical_info_b1  ...  \\\n",
       "0                 202                1               11  ...   \n",
       "1                 263                3               14  ...   \n",
       "2                 325                1               18  ...   \n",
       "3                 213                2               11  ...   \n",
       "4                 102                0               14  ...   \n",
       "...               ...              ...              ...  ...   \n",
       "9995              232                7               17  ...   \n",
       "9996              202                3               19  ...   \n",
       "9997              274                1               11  ...   \n",
       "9998              105                1               13  ...   \n",
       "9999              454                4               13  ...   \n",
       "\n",
       "      medical_keyword_6  medical_keyword_7  medical_keyword_8  \\\n",
       "0                     1                  0                  1   \n",
       "1                     0                  1                  1   \n",
       "2                     1                  0                  1   \n",
       "3                     0                  0                  1   \n",
       "4                     0                  1                  1   \n",
       "...                 ...                ...                ...   \n",
       "9995                  0                  0                  1   \n",
       "9996                  0                  0                  1   \n",
       "9997                  0                  0                  1   \n",
       "9998                  1                  1                  1   \n",
       "9999                  1                  0                  1   \n",
       "\n",
       "      medical_keyword_9  medical_keyword_10  year  month  day  yearmonth  \\\n",
       "0                     0                   0  2015      2    3      24182   \n",
       "1                     0                   0  2015      5    9      24185   \n",
       "2                     0                   0  2016      2   13      24194   \n",
       "3                     0                   0  2015      7    6      24187   \n",
       "4                     1                   0  2016      9   17      24201   \n",
       "...                 ...                 ...   ...    ...  ...        ...   \n",
       "9995                  1                   0  2015     10   21      24190   \n",
       "9996                  1                   0  2015      5   28      24185   \n",
       "9997                  0                   1  2016      2   29      24194   \n",
       "9998                  1                   0  2016      2   27      24194   \n",
       "9999                  0                   0  2015      7    1      24187   \n",
       "\n",
       "      target  \n",
       "0          0  \n",
       "1          0  \n",
       "2          1  \n",
       "3          0  \n",
       "4          1  \n",
       "...      ...  \n",
       "9995       0  \n",
       "9996       0  \n",
       "9997       0  \n",
       "9998       0  \n",
       "9999       0  \n",
       "\n",
       "[10000 rows x 29 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                   0\n",
       "sex                   0\n",
       "height                0\n",
       "weight                0\n",
       "product               0\n",
       "amount                0\n",
       "medical_info_a1       0\n",
       "medical_info_a2       0\n",
       "medical_info_a3       0\n",
       "medical_info_b1       0\n",
       "medical_info_b2       0\n",
       "medical_info_b3       0\n",
       "medical_info_c1       0\n",
       "medical_info_c2       0\n",
       "medical_keyword_1     0\n",
       "medical_keyword_2     0\n",
       "medical_keyword_3     0\n",
       "medical_keyword_4     0\n",
       "medical_keyword_5     0\n",
       "medical_keyword_6     0\n",
       "medical_keyword_7     0\n",
       "medical_keyword_8     0\n",
       "medical_keyword_9     0\n",
       "medical_keyword_10    0\n",
       "year                  0\n",
       "month                 0\n",
       "day                   0\n",
       "yearmonth             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, params=None):\n",
    "        self.model = None\n",
    "        if params is None:\n",
    "           self.params = {}\n",
    "        else:\n",
    "            self.params = params\n",
    "\n",
    "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
    "        params = {'objective': 'binary', 'seed': 71, 'verbose': 0, 'metrics': 'binary_logloss'}\n",
    "        num_round = 100\n",
    "        params.update(self.params)\n",
    "\n",
    "        dtrain = lgb.Dataset(tr_x, tr_y)\n",
    "        dvalid = lgb.Dataset(va_x, va_y)\n",
    "\n",
    "        self.model = lgb.train(\n",
    "            params, dtrain, num_boost_round=num_round, valid_names=[\"train\", \"valid\"], valid_sets=[dtrain, dvalid]\n",
    "        )\n",
    "    def perdict(self, x):\n",
    "        pred = self.model.predict(x)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold-out法での分割\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "tr_x, va_x, tr_y, va_y = train_test_split(train_x, train_y, test_size=0.25, random_state=71, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n",
      "This may cause significantly different results comparing to the previous versions of LightGBM.\n",
      "Try to set boost_from_average=false, if your old models produce bad results\n",
      "[1]\ttrain's binary_logloss: 0.454308\tvalid's binary_logloss: 0.465515\n",
      "[2]\ttrain's binary_logloss: 0.429565\tvalid's binary_logloss: 0.443444\n",
      "[3]\ttrain's binary_logloss: 0.410077\tvalid's binary_logloss: 0.425543\n",
      "[4]\ttrain's binary_logloss: 0.39358\tvalid's binary_logloss: 0.410625\n",
      "[5]\ttrain's binary_logloss: 0.379354\tvalid's binary_logloss: 0.397666\n",
      "[6]\ttrain's binary_logloss: 0.365913\tvalid's binary_logloss: 0.387422\n",
      "[7]\ttrain's binary_logloss: 0.354309\tvalid's binary_logloss: 0.376037\n",
      "[8]\ttrain's binary_logloss: 0.344354\tvalid's binary_logloss: 0.366734\n",
      "[9]\ttrain's binary_logloss: 0.334834\tvalid's binary_logloss: 0.35898\n",
      "[10]\ttrain's binary_logloss: 0.326209\tvalid's binary_logloss: 0.351612\n",
      "[11]\ttrain's binary_logloss: 0.317809\tvalid's binary_logloss: 0.34563\n",
      "[12]\ttrain's binary_logloss: 0.310845\tvalid's binary_logloss: 0.340564\n",
      "[13]\ttrain's binary_logloss: 0.30401\tvalid's binary_logloss: 0.334274\n",
      "[14]\ttrain's binary_logloss: 0.296333\tvalid's binary_logloss: 0.327911\n",
      "[15]\ttrain's binary_logloss: 0.290137\tvalid's binary_logloss: 0.324239\n",
      "[16]\ttrain's binary_logloss: 0.283293\tvalid's binary_logloss: 0.317865\n",
      "[17]\ttrain's binary_logloss: 0.277823\tvalid's binary_logloss: 0.314412\n",
      "[18]\ttrain's binary_logloss: 0.272322\tvalid's binary_logloss: 0.310773\n",
      "[19]\ttrain's binary_logloss: 0.26705\tvalid's binary_logloss: 0.307374\n",
      "[20]\ttrain's binary_logloss: 0.262091\tvalid's binary_logloss: 0.303946\n",
      "[21]\ttrain's binary_logloss: 0.257394\tvalid's binary_logloss: 0.301154\n",
      "[22]\ttrain's binary_logloss: 0.253115\tvalid's binary_logloss: 0.298085\n",
      "[23]\ttrain's binary_logloss: 0.24878\tvalid's binary_logloss: 0.294533\n",
      "[24]\ttrain's binary_logloss: 0.243951\tvalid's binary_logloss: 0.291832\n",
      "[25]\ttrain's binary_logloss: 0.240469\tvalid's binary_logloss: 0.289538\n",
      "[26]\ttrain's binary_logloss: 0.236038\tvalid's binary_logloss: 0.285777\n",
      "[27]\ttrain's binary_logloss: 0.231149\tvalid's binary_logloss: 0.281554\n",
      "[28]\ttrain's binary_logloss: 0.227936\tvalid's binary_logloss: 0.279469\n",
      "[29]\ttrain's binary_logloss: 0.224501\tvalid's binary_logloss: 0.278255\n",
      "[30]\ttrain's binary_logloss: 0.221476\tvalid's binary_logloss: 0.27687\n",
      "[31]\ttrain's binary_logloss: 0.218174\tvalid's binary_logloss: 0.274434\n",
      "[32]\ttrain's binary_logloss: 0.214462\tvalid's binary_logloss: 0.272274\n",
      "[33]\ttrain's binary_logloss: 0.21088\tvalid's binary_logloss: 0.269122\n",
      "[34]\ttrain's binary_logloss: 0.207684\tvalid's binary_logloss: 0.266756\n",
      "[35]\ttrain's binary_logloss: 0.205168\tvalid's binary_logloss: 0.265291\n",
      "[36]\ttrain's binary_logloss: 0.202792\tvalid's binary_logloss: 0.264635\n",
      "[37]\ttrain's binary_logloss: 0.200275\tvalid's binary_logloss: 0.263032\n",
      "[38]\ttrain's binary_logloss: 0.19745\tvalid's binary_logloss: 0.261202\n",
      "[39]\ttrain's binary_logloss: 0.194507\tvalid's binary_logloss: 0.25904\n",
      "[40]\ttrain's binary_logloss: 0.192425\tvalid's binary_logloss: 0.257587\n",
      "[41]\ttrain's binary_logloss: 0.190121\tvalid's binary_logloss: 0.256767\n",
      "[42]\ttrain's binary_logloss: 0.187834\tvalid's binary_logloss: 0.255625\n",
      "[43]\ttrain's binary_logloss: 0.184909\tvalid's binary_logloss: 0.254088\n",
      "[44]\ttrain's binary_logloss: 0.181938\tvalid's binary_logloss: 0.252355\n",
      "[45]\ttrain's binary_logloss: 0.180014\tvalid's binary_logloss: 0.251427\n",
      "[46]\ttrain's binary_logloss: 0.177952\tvalid's binary_logloss: 0.250039\n",
      "[47]\ttrain's binary_logloss: 0.175486\tvalid's binary_logloss: 0.248749\n",
      "[48]\ttrain's binary_logloss: 0.17332\tvalid's binary_logloss: 0.247716\n",
      "[49]\ttrain's binary_logloss: 0.171233\tvalid's binary_logloss: 0.247249\n",
      "[50]\ttrain's binary_logloss: 0.168808\tvalid's binary_logloss: 0.245712\n",
      "[51]\ttrain's binary_logloss: 0.166901\tvalid's binary_logloss: 0.244631\n",
      "[52]\ttrain's binary_logloss: 0.164701\tvalid's binary_logloss: 0.243459\n",
      "[53]\ttrain's binary_logloss: 0.162987\tvalid's binary_logloss: 0.242513\n",
      "[54]\ttrain's binary_logloss: 0.161101\tvalid's binary_logloss: 0.24179\n",
      "[55]\ttrain's binary_logloss: 0.159227\tvalid's binary_logloss: 0.240958\n",
      "[56]\ttrain's binary_logloss: 0.157179\tvalid's binary_logloss: 0.23982\n",
      "[57]\ttrain's binary_logloss: 0.155645\tvalid's binary_logloss: 0.239127\n",
      "[58]\ttrain's binary_logloss: 0.153637\tvalid's binary_logloss: 0.237746\n",
      "[59]\ttrain's binary_logloss: 0.152156\tvalid's binary_logloss: 0.237403\n",
      "[60]\ttrain's binary_logloss: 0.150492\tvalid's binary_logloss: 0.236752\n",
      "[61]\ttrain's binary_logloss: 0.14833\tvalid's binary_logloss: 0.235299\n",
      "[62]\ttrain's binary_logloss: 0.146708\tvalid's binary_logloss: 0.234711\n",
      "[63]\ttrain's binary_logloss: 0.145146\tvalid's binary_logloss: 0.234231\n",
      "[64]\ttrain's binary_logloss: 0.143475\tvalid's binary_logloss: 0.233571\n",
      "[65]\ttrain's binary_logloss: 0.141857\tvalid's binary_logloss: 0.233159\n",
      "[66]\ttrain's binary_logloss: 0.140617\tvalid's binary_logloss: 0.232806\n",
      "[67]\ttrain's binary_logloss: 0.139349\tvalid's binary_logloss: 0.232361\n",
      "[68]\ttrain's binary_logloss: 0.137656\tvalid's binary_logloss: 0.231805\n",
      "[69]\ttrain's binary_logloss: 0.13622\tvalid's binary_logloss: 0.231595\n",
      "[70]\ttrain's binary_logloss: 0.134909\tvalid's binary_logloss: 0.231123\n",
      "[71]\ttrain's binary_logloss: 0.133365\tvalid's binary_logloss: 0.23016\n",
      "[72]\ttrain's binary_logloss: 0.131928\tvalid's binary_logloss: 0.22954\n",
      "[73]\ttrain's binary_logloss: 0.130743\tvalid's binary_logloss: 0.2294\n",
      "[74]\ttrain's binary_logloss: 0.129119\tvalid's binary_logloss: 0.228424\n",
      "[75]\ttrain's binary_logloss: 0.127482\tvalid's binary_logloss: 0.227763\n",
      "[76]\ttrain's binary_logloss: 0.126147\tvalid's binary_logloss: 0.227717\n",
      "[77]\ttrain's binary_logloss: 0.124614\tvalid's binary_logloss: 0.226361\n",
      "[78]\ttrain's binary_logloss: 0.123468\tvalid's binary_logloss: 0.226123\n",
      "[79]\ttrain's binary_logloss: 0.121894\tvalid's binary_logloss: 0.224743\n",
      "[80]\ttrain's binary_logloss: 0.120732\tvalid's binary_logloss: 0.224614\n",
      "[81]\ttrain's binary_logloss: 0.119582\tvalid's binary_logloss: 0.224197\n",
      "[82]\ttrain's binary_logloss: 0.118468\tvalid's binary_logloss: 0.223638\n",
      "[83]\ttrain's binary_logloss: 0.117333\tvalid's binary_logloss: 0.223198\n",
      "[84]\ttrain's binary_logloss: 0.116301\tvalid's binary_logloss: 0.223096\n",
      "[85]\ttrain's binary_logloss: 0.11519\tvalid's binary_logloss: 0.222564\n",
      "[86]\ttrain's binary_logloss: 0.114083\tvalid's binary_logloss: 0.222174\n",
      "[87]\ttrain's binary_logloss: 0.112979\tvalid's binary_logloss: 0.222011\n",
      "[88]\ttrain's binary_logloss: 0.11165\tvalid's binary_logloss: 0.220694\n",
      "[89]\ttrain's binary_logloss: 0.110383\tvalid's binary_logloss: 0.220025\n",
      "[90]\ttrain's binary_logloss: 0.109327\tvalid's binary_logloss: 0.219546\n",
      "[91]\ttrain's binary_logloss: 0.108284\tvalid's binary_logloss: 0.218836\n",
      "[92]\ttrain's binary_logloss: 0.107136\tvalid's binary_logloss: 0.21845\n",
      "[93]\ttrain's binary_logloss: 0.106131\tvalid's binary_logloss: 0.218435\n",
      "[94]\ttrain's binary_logloss: 0.105149\tvalid's binary_logloss: 0.218246\n",
      "[95]\ttrain's binary_logloss: 0.104193\tvalid's binary_logloss: 0.218185\n",
      "[96]\ttrain's binary_logloss: 0.103162\tvalid's binary_logloss: 0.217907\n",
      "[97]\ttrain's binary_logloss: 0.102197\tvalid's binary_logloss: 0.217536\n",
      "[98]\ttrain's binary_logloss: 0.101254\tvalid's binary_logloss: 0.217335\n",
      "[99]\ttrain's binary_logloss: 0.100266\tvalid's binary_logloss: 0.217277\n",
      "[100]\ttrain's binary_logloss: 0.0994527\tvalid's binary_logloss: 0.217264\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.fit(tr_x, tr_y, va_x, va_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00738854, 0.38913896, 0.00381021, ..., 0.1096412 , 0.08484495,\n",
       "       0.00803791])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va_pred = model.perdict(va_x)\n",
    "va_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21726392718295187"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = log_loss(va_y, va_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n",
      "This may cause significantly different results comparing to the previous versions of LightGBM.\n",
      "Try to set boost_from_average=false, if your old models produce bad results\n",
      "[1]\ttrain's binary_logloss: 0.454308\tvalid's binary_logloss: 0.465515\n",
      "[2]\ttrain's binary_logloss: 0.429565\tvalid's binary_logloss: 0.443444\n",
      "[3]\ttrain's binary_logloss: 0.410077\tvalid's binary_logloss: 0.425543\n",
      "[4]\ttrain's binary_logloss: 0.39358\tvalid's binary_logloss: 0.410625\n",
      "[5]\ttrain's binary_logloss: 0.379354\tvalid's binary_logloss: 0.397666\n",
      "[6]\ttrain's binary_logloss: 0.365913\tvalid's binary_logloss: 0.387422\n",
      "[7]\ttrain's binary_logloss: 0.354309\tvalid's binary_logloss: 0.376037\n",
      "[8]\ttrain's binary_logloss: 0.344354\tvalid's binary_logloss: 0.366734\n",
      "[9]\ttrain's binary_logloss: 0.334834\tvalid's binary_logloss: 0.35898\n",
      "[10]\ttrain's binary_logloss: 0.326209\tvalid's binary_logloss: 0.351612\n",
      "[11]\ttrain's binary_logloss: 0.317809\tvalid's binary_logloss: 0.34563\n",
      "[12]\ttrain's binary_logloss: 0.310845\tvalid's binary_logloss: 0.340564\n",
      "[13]\ttrain's binary_logloss: 0.30401\tvalid's binary_logloss: 0.334274\n",
      "[14]\ttrain's binary_logloss: 0.296333\tvalid's binary_logloss: 0.327911\n",
      "[15]\ttrain's binary_logloss: 0.290137\tvalid's binary_logloss: 0.324239\n",
      "[16]\ttrain's binary_logloss: 0.283293\tvalid's binary_logloss: 0.317865\n",
      "[17]\ttrain's binary_logloss: 0.277823\tvalid's binary_logloss: 0.314412\n",
      "[18]\ttrain's binary_logloss: 0.272322\tvalid's binary_logloss: 0.310773\n",
      "[19]\ttrain's binary_logloss: 0.26705\tvalid's binary_logloss: 0.307374\n",
      "[20]\ttrain's binary_logloss: 0.262091\tvalid's binary_logloss: 0.303946\n",
      "[21]\ttrain's binary_logloss: 0.257394\tvalid's binary_logloss: 0.301154\n",
      "[22]\ttrain's binary_logloss: 0.253115\tvalid's binary_logloss: 0.298085\n",
      "[23]\ttrain's binary_logloss: 0.24878\tvalid's binary_logloss: 0.294533\n",
      "[24]\ttrain's binary_logloss: 0.243951\tvalid's binary_logloss: 0.291832\n",
      "[25]\ttrain's binary_logloss: 0.240469\tvalid's binary_logloss: 0.289538\n",
      "[26]\ttrain's binary_logloss: 0.236038\tvalid's binary_logloss: 0.285777\n",
      "[27]\ttrain's binary_logloss: 0.231149\tvalid's binary_logloss: 0.281554\n",
      "[28]\ttrain's binary_logloss: 0.227936\tvalid's binary_logloss: 0.279469\n",
      "[29]\ttrain's binary_logloss: 0.224501\tvalid's binary_logloss: 0.278255\n",
      "[30]\ttrain's binary_logloss: 0.221476\tvalid's binary_logloss: 0.27687\n",
      "[31]\ttrain's binary_logloss: 0.218174\tvalid's binary_logloss: 0.274434\n",
      "[32]\ttrain's binary_logloss: 0.214462\tvalid's binary_logloss: 0.272274\n",
      "[33]\ttrain's binary_logloss: 0.21088\tvalid's binary_logloss: 0.269122\n",
      "[34]\ttrain's binary_logloss: 0.207684\tvalid's binary_logloss: 0.266756\n",
      "[35]\ttrain's binary_logloss: 0.205168\tvalid's binary_logloss: 0.265291\n",
      "[36]\ttrain's binary_logloss: 0.202792\tvalid's binary_logloss: 0.264635\n",
      "[37]\ttrain's binary_logloss: 0.200275\tvalid's binary_logloss: 0.263032\n",
      "[38]\ttrain's binary_logloss: 0.19745\tvalid's binary_logloss: 0.261202\n",
      "[39]\ttrain's binary_logloss: 0.194507\tvalid's binary_logloss: 0.25904\n",
      "[40]\ttrain's binary_logloss: 0.192425\tvalid's binary_logloss: 0.257587\n",
      "[41]\ttrain's binary_logloss: 0.190121\tvalid's binary_logloss: 0.256767\n",
      "[42]\ttrain's binary_logloss: 0.187834\tvalid's binary_logloss: 0.255625\n",
      "[43]\ttrain's binary_logloss: 0.184909\tvalid's binary_logloss: 0.254088\n",
      "[44]\ttrain's binary_logloss: 0.181938\tvalid's binary_logloss: 0.252355\n",
      "[45]\ttrain's binary_logloss: 0.180014\tvalid's binary_logloss: 0.251427\n",
      "[46]\ttrain's binary_logloss: 0.177952\tvalid's binary_logloss: 0.250039\n",
      "[47]\ttrain's binary_logloss: 0.175486\tvalid's binary_logloss: 0.248749\n",
      "[48]\ttrain's binary_logloss: 0.17332\tvalid's binary_logloss: 0.247716\n",
      "[49]\ttrain's binary_logloss: 0.171233\tvalid's binary_logloss: 0.247249\n",
      "[50]\ttrain's binary_logloss: 0.168808\tvalid's binary_logloss: 0.245712\n",
      "[51]\ttrain's binary_logloss: 0.166901\tvalid's binary_logloss: 0.244631\n",
      "[52]\ttrain's binary_logloss: 0.164701\tvalid's binary_logloss: 0.243459\n",
      "[53]\ttrain's binary_logloss: 0.162987\tvalid's binary_logloss: 0.242513\n",
      "[54]\ttrain's binary_logloss: 0.161101\tvalid's binary_logloss: 0.24179\n",
      "[55]\ttrain's binary_logloss: 0.159227\tvalid's binary_logloss: 0.240958\n",
      "[56]\ttrain's binary_logloss: 0.157179\tvalid's binary_logloss: 0.23982\n",
      "[57]\ttrain's binary_logloss: 0.155645\tvalid's binary_logloss: 0.239127\n",
      "[58]\ttrain's binary_logloss: 0.153637\tvalid's binary_logloss: 0.237746\n",
      "[59]\ttrain's binary_logloss: 0.152156\tvalid's binary_logloss: 0.237403\n",
      "[60]\ttrain's binary_logloss: 0.150492\tvalid's binary_logloss: 0.236752\n",
      "[61]\ttrain's binary_logloss: 0.14833\tvalid's binary_logloss: 0.235299\n",
      "[62]\ttrain's binary_logloss: 0.146708\tvalid's binary_logloss: 0.234711\n",
      "[63]\ttrain's binary_logloss: 0.145146\tvalid's binary_logloss: 0.234231\n",
      "[64]\ttrain's binary_logloss: 0.143475\tvalid's binary_logloss: 0.233571\n",
      "[65]\ttrain's binary_logloss: 0.141857\tvalid's binary_logloss: 0.233159\n",
      "[66]\ttrain's binary_logloss: 0.140617\tvalid's binary_logloss: 0.232806\n",
      "[67]\ttrain's binary_logloss: 0.139349\tvalid's binary_logloss: 0.232361\n",
      "[68]\ttrain's binary_logloss: 0.137656\tvalid's binary_logloss: 0.231805\n",
      "[69]\ttrain's binary_logloss: 0.13622\tvalid's binary_logloss: 0.231595\n",
      "[70]\ttrain's binary_logloss: 0.134909\tvalid's binary_logloss: 0.231123\n",
      "[71]\ttrain's binary_logloss: 0.133365\tvalid's binary_logloss: 0.23016\n",
      "[72]\ttrain's binary_logloss: 0.131928\tvalid's binary_logloss: 0.22954\n",
      "[73]\ttrain's binary_logloss: 0.130743\tvalid's binary_logloss: 0.2294\n",
      "[74]\ttrain's binary_logloss: 0.129119\tvalid's binary_logloss: 0.228424\n",
      "[75]\ttrain's binary_logloss: 0.127482\tvalid's binary_logloss: 0.227763\n",
      "[76]\ttrain's binary_logloss: 0.126147\tvalid's binary_logloss: 0.227717\n",
      "[77]\ttrain's binary_logloss: 0.124614\tvalid's binary_logloss: 0.226361\n",
      "[78]\ttrain's binary_logloss: 0.123468\tvalid's binary_logloss: 0.226123\n",
      "[79]\ttrain's binary_logloss: 0.121894\tvalid's binary_logloss: 0.224743\n",
      "[80]\ttrain's binary_logloss: 0.120732\tvalid's binary_logloss: 0.224614\n",
      "[81]\ttrain's binary_logloss: 0.119582\tvalid's binary_logloss: 0.224197\n",
      "[82]\ttrain's binary_logloss: 0.118468\tvalid's binary_logloss: 0.223638\n",
      "[83]\ttrain's binary_logloss: 0.117333\tvalid's binary_logloss: 0.223198\n",
      "[84]\ttrain's binary_logloss: 0.116301\tvalid's binary_logloss: 0.223096\n",
      "[85]\ttrain's binary_logloss: 0.11519\tvalid's binary_logloss: 0.222564\n",
      "[86]\ttrain's binary_logloss: 0.114083\tvalid's binary_logloss: 0.222174\n",
      "[87]\ttrain's binary_logloss: 0.112979\tvalid's binary_logloss: 0.222011\n",
      "[88]\ttrain's binary_logloss: 0.11165\tvalid's binary_logloss: 0.220694\n",
      "[89]\ttrain's binary_logloss: 0.110383\tvalid's binary_logloss: 0.220025\n",
      "[90]\ttrain's binary_logloss: 0.109327\tvalid's binary_logloss: 0.219546\n",
      "[91]\ttrain's binary_logloss: 0.108284\tvalid's binary_logloss: 0.218836\n",
      "[92]\ttrain's binary_logloss: 0.107136\tvalid's binary_logloss: 0.21845\n",
      "[93]\ttrain's binary_logloss: 0.106131\tvalid's binary_logloss: 0.218435\n",
      "[94]\ttrain's binary_logloss: 0.105149\tvalid's binary_logloss: 0.218246\n",
      "[95]\ttrain's binary_logloss: 0.104193\tvalid's binary_logloss: 0.218185\n",
      "[96]\ttrain's binary_logloss: 0.103162\tvalid's binary_logloss: 0.217907\n",
      "[97]\ttrain's binary_logloss: 0.102197\tvalid's binary_logloss: 0.217536\n",
      "[98]\ttrain's binary_logloss: 0.101254\tvalid's binary_logloss: 0.217335\n",
      "[99]\ttrain's binary_logloss: 0.100266\tvalid's binary_logloss: 0.217277\n",
      "[100]\ttrain's binary_logloss: 0.0994527\tvalid's binary_logloss: 0.217264\n",
      "[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n",
      "This may cause significantly different results comparing to the previous versions of LightGBM.\n",
      "Try to set boost_from_average=false, if your old models produce bad results\n",
      "[1]\ttrain's binary_logloss: 0.460683\tvalid's binary_logloss: 0.445527\n",
      "[2]\ttrain's binary_logloss: 0.434914\tvalid's binary_logloss: 0.42475\n",
      "[3]\ttrain's binary_logloss: 0.414154\tvalid's binary_logloss: 0.408448\n",
      "[4]\ttrain's binary_logloss: 0.396952\tvalid's binary_logloss: 0.394103\n",
      "[5]\ttrain's binary_logloss: 0.38195\tvalid's binary_logloss: 0.382367\n",
      "[6]\ttrain's binary_logloss: 0.369146\tvalid's binary_logloss: 0.372506\n",
      "[7]\ttrain's binary_logloss: 0.357112\tvalid's binary_logloss: 0.364028\n",
      "[8]\ttrain's binary_logloss: 0.346977\tvalid's binary_logloss: 0.357331\n",
      "[9]\ttrain's binary_logloss: 0.337622\tvalid's binary_logloss: 0.350197\n",
      "[10]\ttrain's binary_logloss: 0.329478\tvalid's binary_logloss: 0.344383\n",
      "[11]\ttrain's binary_logloss: 0.321132\tvalid's binary_logloss: 0.338466\n",
      "[12]\ttrain's binary_logloss: 0.313617\tvalid's binary_logloss: 0.332766\n",
      "[13]\ttrain's binary_logloss: 0.30608\tvalid's binary_logloss: 0.326439\n",
      "[14]\ttrain's binary_logloss: 0.299119\tvalid's binary_logloss: 0.321705\n",
      "[15]\ttrain's binary_logloss: 0.292493\tvalid's binary_logloss: 0.316989\n",
      "[16]\ttrain's binary_logloss: 0.285813\tvalid's binary_logloss: 0.311977\n",
      "[17]\ttrain's binary_logloss: 0.27985\tvalid's binary_logloss: 0.308307\n",
      "[18]\ttrain's binary_logloss: 0.274492\tvalid's binary_logloss: 0.305441\n",
      "[19]\ttrain's binary_logloss: 0.269809\tvalid's binary_logloss: 0.301833\n",
      "[20]\ttrain's binary_logloss: 0.264112\tvalid's binary_logloss: 0.298228\n",
      "[21]\ttrain's binary_logloss: 0.259142\tvalid's binary_logloss: 0.294624\n",
      "[22]\ttrain's binary_logloss: 0.254988\tvalid's binary_logloss: 0.291626\n",
      "[23]\ttrain's binary_logloss: 0.2509\tvalid's binary_logloss: 0.289124\n",
      "[24]\ttrain's binary_logloss: 0.246887\tvalid's binary_logloss: 0.28657\n",
      "[25]\ttrain's binary_logloss: 0.24273\tvalid's binary_logloss: 0.284375\n",
      "[26]\ttrain's binary_logloss: 0.238828\tvalid's binary_logloss: 0.282064\n",
      "[27]\ttrain's binary_logloss: 0.234758\tvalid's binary_logloss: 0.279011\n",
      "[28]\ttrain's binary_logloss: 0.231367\tvalid's binary_logloss: 0.27719\n",
      "[29]\ttrain's binary_logloss: 0.226885\tvalid's binary_logloss: 0.27392\n",
      "[30]\ttrain's binary_logloss: 0.223622\tvalid's binary_logloss: 0.271819\n",
      "[31]\ttrain's binary_logloss: 0.220823\tvalid's binary_logloss: 0.26999\n",
      "[32]\ttrain's binary_logloss: 0.218027\tvalid's binary_logloss: 0.268667\n",
      "[33]\ttrain's binary_logloss: 0.214645\tvalid's binary_logloss: 0.266851\n",
      "[34]\ttrain's binary_logloss: 0.21106\tvalid's binary_logloss: 0.263664\n",
      "[35]\ttrain's binary_logloss: 0.208429\tvalid's binary_logloss: 0.262366\n",
      "[36]\ttrain's binary_logloss: 0.205435\tvalid's binary_logloss: 0.260092\n",
      "[37]\ttrain's binary_logloss: 0.202718\tvalid's binary_logloss: 0.258452\n",
      "[38]\ttrain's binary_logloss: 0.199477\tvalid's binary_logloss: 0.255889\n",
      "[39]\ttrain's binary_logloss: 0.196729\tvalid's binary_logloss: 0.254161\n",
      "[40]\ttrain's binary_logloss: 0.193862\tvalid's binary_logloss: 0.252688\n",
      "[41]\ttrain's binary_logloss: 0.191261\tvalid's binary_logloss: 0.250924\n",
      "[42]\ttrain's binary_logloss: 0.188936\tvalid's binary_logloss: 0.250589\n",
      "[43]\ttrain's binary_logloss: 0.186726\tvalid's binary_logloss: 0.249751\n",
      "[44]\ttrain's binary_logloss: 0.184547\tvalid's binary_logloss: 0.248667\n",
      "[45]\ttrain's binary_logloss: 0.182229\tvalid's binary_logloss: 0.247304\n",
      "[46]\ttrain's binary_logloss: 0.180278\tvalid's binary_logloss: 0.24663\n",
      "[47]\ttrain's binary_logloss: 0.178227\tvalid's binary_logloss: 0.245724\n",
      "[48]\ttrain's binary_logloss: 0.17598\tvalid's binary_logloss: 0.244177\n",
      "[49]\ttrain's binary_logloss: 0.174074\tvalid's binary_logloss: 0.243811\n",
      "[50]\ttrain's binary_logloss: 0.171338\tvalid's binary_logloss: 0.24192\n",
      "[51]\ttrain's binary_logloss: 0.169476\tvalid's binary_logloss: 0.241339\n",
      "[52]\ttrain's binary_logloss: 0.167169\tvalid's binary_logloss: 0.239601\n",
      "[53]\ttrain's binary_logloss: 0.165316\tvalid's binary_logloss: 0.238659\n",
      "[54]\ttrain's binary_logloss: 0.163015\tvalid's binary_logloss: 0.236957\n",
      "[55]\ttrain's binary_logloss: 0.160341\tvalid's binary_logloss: 0.234712\n",
      "[56]\ttrain's binary_logloss: 0.158558\tvalid's binary_logloss: 0.234069\n",
      "[57]\ttrain's binary_logloss: 0.156989\tvalid's binary_logloss: 0.23333\n",
      "[58]\ttrain's binary_logloss: 0.155397\tvalid's binary_logloss: 0.2327\n",
      "[59]\ttrain's binary_logloss: 0.153571\tvalid's binary_logloss: 0.231853\n",
      "[60]\ttrain's binary_logloss: 0.151882\tvalid's binary_logloss: 0.231119\n",
      "[61]\ttrain's binary_logloss: 0.150035\tvalid's binary_logloss: 0.229544\n",
      "[62]\ttrain's binary_logloss: 0.148515\tvalid's binary_logloss: 0.229169\n",
      "[63]\ttrain's binary_logloss: 0.146908\tvalid's binary_logloss: 0.228923\n",
      "[64]\ttrain's binary_logloss: 0.145537\tvalid's binary_logloss: 0.228752\n",
      "[65]\ttrain's binary_logloss: 0.144203\tvalid's binary_logloss: 0.228824\n",
      "[66]\ttrain's binary_logloss: 0.1428\tvalid's binary_logloss: 0.228232\n",
      "[67]\ttrain's binary_logloss: 0.141316\tvalid's binary_logloss: 0.227693\n",
      "[68]\ttrain's binary_logloss: 0.139916\tvalid's binary_logloss: 0.226982\n",
      "[69]\ttrain's binary_logloss: 0.138496\tvalid's binary_logloss: 0.226912\n",
      "[70]\ttrain's binary_logloss: 0.136849\tvalid's binary_logloss: 0.225778\n",
      "[71]\ttrain's binary_logloss: 0.135715\tvalid's binary_logloss: 0.225521\n",
      "[72]\ttrain's binary_logloss: 0.13433\tvalid's binary_logloss: 0.224983\n",
      "[73]\ttrain's binary_logloss: 0.132912\tvalid's binary_logloss: 0.224385\n",
      "[74]\ttrain's binary_logloss: 0.13138\tvalid's binary_logloss: 0.223778\n",
      "[75]\ttrain's binary_logloss: 0.130124\tvalid's binary_logloss: 0.222861\n",
      "[76]\ttrain's binary_logloss: 0.128931\tvalid's binary_logloss: 0.222522\n",
      "[77]\ttrain's binary_logloss: 0.127775\tvalid's binary_logloss: 0.222622\n",
      "[78]\ttrain's binary_logloss: 0.126455\tvalid's binary_logloss: 0.222063\n",
      "[79]\ttrain's binary_logloss: 0.124851\tvalid's binary_logloss: 0.221238\n",
      "[80]\ttrain's binary_logloss: 0.123553\tvalid's binary_logloss: 0.220734\n",
      "[81]\ttrain's binary_logloss: 0.122352\tvalid's binary_logloss: 0.220331\n",
      "[82]\ttrain's binary_logloss: 0.121223\tvalid's binary_logloss: 0.219743\n",
      "[83]\ttrain's binary_logloss: 0.1202\tvalid's binary_logloss: 0.220056\n",
      "[84]\ttrain's binary_logloss: 0.119073\tvalid's binary_logloss: 0.219459\n",
      "[85]\ttrain's binary_logloss: 0.117989\tvalid's binary_logloss: 0.219275\n",
      "[86]\ttrain's binary_logloss: 0.116666\tvalid's binary_logloss: 0.218472\n",
      "[87]\ttrain's binary_logloss: 0.115579\tvalid's binary_logloss: 0.218394\n",
      "[88]\ttrain's binary_logloss: 0.114462\tvalid's binary_logloss: 0.217985\n",
      "[89]\ttrain's binary_logloss: 0.113412\tvalid's binary_logloss: 0.217221\n",
      "[90]\ttrain's binary_logloss: 0.112221\tvalid's binary_logloss: 0.217117\n",
      "[91]\ttrain's binary_logloss: 0.111297\tvalid's binary_logloss: 0.2171\n",
      "[92]\ttrain's binary_logloss: 0.110277\tvalid's binary_logloss: 0.216819\n",
      "[93]\ttrain's binary_logloss: 0.109143\tvalid's binary_logloss: 0.216398\n",
      "[94]\ttrain's binary_logloss: 0.107932\tvalid's binary_logloss: 0.215407\n",
      "[95]\ttrain's binary_logloss: 0.106959\tvalid's binary_logloss: 0.215293\n",
      "[96]\ttrain's binary_logloss: 0.106024\tvalid's binary_logloss: 0.214868\n",
      "[97]\ttrain's binary_logloss: 0.105188\tvalid's binary_logloss: 0.214679\n",
      "[98]\ttrain's binary_logloss: 0.104204\tvalid's binary_logloss: 0.214062\n",
      "[99]\ttrain's binary_logloss: 0.103322\tvalid's binary_logloss: 0.214188\n",
      "[100]\ttrain's binary_logloss: 0.102367\tvalid's binary_logloss: 0.214592\n",
      "[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n",
      "This may cause significantly different results comparing to the previous versions of LightGBM.\n",
      "Try to set boost_from_average=false, if your old models produce bad results\n",
      "[1]\ttrain's binary_logloss: 0.454514\tvalid's binary_logloss: 0.462302\n",
      "[2]\ttrain's binary_logloss: 0.430155\tvalid's binary_logloss: 0.439446\n",
      "[3]\ttrain's binary_logloss: 0.411274\tvalid's binary_logloss: 0.420192\n",
      "[4]\ttrain's binary_logloss: 0.395891\tvalid's binary_logloss: 0.405234\n",
      "[5]\ttrain's binary_logloss: 0.380893\tvalid's binary_logloss: 0.391113\n",
      "[6]\ttrain's binary_logloss: 0.3687\tvalid's binary_logloss: 0.381125\n",
      "[7]\ttrain's binary_logloss: 0.357678\tvalid's binary_logloss: 0.371047\n",
      "[8]\ttrain's binary_logloss: 0.347273\tvalid's binary_logloss: 0.362708\n",
      "[9]\ttrain's binary_logloss: 0.3384\tvalid's binary_logloss: 0.355273\n",
      "[10]\ttrain's binary_logloss: 0.329459\tvalid's binary_logloss: 0.346679\n",
      "[11]\ttrain's binary_logloss: 0.321347\tvalid's binary_logloss: 0.340116\n",
      "[12]\ttrain's binary_logloss: 0.314839\tvalid's binary_logloss: 0.334649\n",
      "[13]\ttrain's binary_logloss: 0.308432\tvalid's binary_logloss: 0.328457\n",
      "[14]\ttrain's binary_logloss: 0.302163\tvalid's binary_logloss: 0.323778\n",
      "[15]\ttrain's binary_logloss: 0.295853\tvalid's binary_logloss: 0.318936\n",
      "[16]\ttrain's binary_logloss: 0.289292\tvalid's binary_logloss: 0.314108\n",
      "[17]\ttrain's binary_logloss: 0.28355\tvalid's binary_logloss: 0.309519\n",
      "[18]\ttrain's binary_logloss: 0.278249\tvalid's binary_logloss: 0.305123\n",
      "[19]\ttrain's binary_logloss: 0.27248\tvalid's binary_logloss: 0.299955\n",
      "[20]\ttrain's binary_logloss: 0.267242\tvalid's binary_logloss: 0.295732\n",
      "[21]\ttrain's binary_logloss: 0.262732\tvalid's binary_logloss: 0.292204\n",
      "[22]\ttrain's binary_logloss: 0.258047\tvalid's binary_logloss: 0.289008\n",
      "[23]\ttrain's binary_logloss: 0.252391\tvalid's binary_logloss: 0.284907\n",
      "[24]\ttrain's binary_logloss: 0.247533\tvalid's binary_logloss: 0.281473\n",
      "[25]\ttrain's binary_logloss: 0.243537\tvalid's binary_logloss: 0.278999\n",
      "[26]\ttrain's binary_logloss: 0.238803\tvalid's binary_logloss: 0.275479\n",
      "[27]\ttrain's binary_logloss: 0.235009\tvalid's binary_logloss: 0.27302\n",
      "[28]\ttrain's binary_logloss: 0.230733\tvalid's binary_logloss: 0.27064\n",
      "[29]\ttrain's binary_logloss: 0.226691\tvalid's binary_logloss: 0.267774\n",
      "[30]\ttrain's binary_logloss: 0.222935\tvalid's binary_logloss: 0.265414\n",
      "[31]\ttrain's binary_logloss: 0.220249\tvalid's binary_logloss: 0.264213\n",
      "[32]\ttrain's binary_logloss: 0.217233\tvalid's binary_logloss: 0.262515\n",
      "[33]\ttrain's binary_logloss: 0.213647\tvalid's binary_logloss: 0.260202\n",
      "[34]\ttrain's binary_logloss: 0.210791\tvalid's binary_logloss: 0.259205\n",
      "[35]\ttrain's binary_logloss: 0.208228\tvalid's binary_logloss: 0.258056\n",
      "[36]\ttrain's binary_logloss: 0.205292\tvalid's binary_logloss: 0.256281\n",
      "[37]\ttrain's binary_logloss: 0.201464\tvalid's binary_logloss: 0.253669\n",
      "[38]\ttrain's binary_logloss: 0.199071\tvalid's binary_logloss: 0.2523\n",
      "[39]\ttrain's binary_logloss: 0.196644\tvalid's binary_logloss: 0.250643\n",
      "[40]\ttrain's binary_logloss: 0.193564\tvalid's binary_logloss: 0.249192\n",
      "[41]\ttrain's binary_logloss: 0.191324\tvalid's binary_logloss: 0.247782\n",
      "[42]\ttrain's binary_logloss: 0.188767\tvalid's binary_logloss: 0.246162\n",
      "[43]\ttrain's binary_logloss: 0.186339\tvalid's binary_logloss: 0.244493\n",
      "[44]\ttrain's binary_logloss: 0.183478\tvalid's binary_logloss: 0.243403\n",
      "[45]\ttrain's binary_logloss: 0.181354\tvalid's binary_logloss: 0.242426\n",
      "[46]\ttrain's binary_logloss: 0.179377\tvalid's binary_logloss: 0.241998\n",
      "[47]\ttrain's binary_logloss: 0.177363\tvalid's binary_logloss: 0.240868\n",
      "[48]\ttrain's binary_logloss: 0.175493\tvalid's binary_logloss: 0.240088\n",
      "[49]\ttrain's binary_logloss: 0.173663\tvalid's binary_logloss: 0.238919\n",
      "[50]\ttrain's binary_logloss: 0.170515\tvalid's binary_logloss: 0.236455\n",
      "[51]\ttrain's binary_logloss: 0.168623\tvalid's binary_logloss: 0.235594\n",
      "[52]\ttrain's binary_logloss: 0.166865\tvalid's binary_logloss: 0.234834\n",
      "[53]\ttrain's binary_logloss: 0.165164\tvalid's binary_logloss: 0.234259\n",
      "[54]\ttrain's binary_logloss: 0.163417\tvalid's binary_logloss: 0.233238\n",
      "[55]\ttrain's binary_logloss: 0.161513\tvalid's binary_logloss: 0.232266\n",
      "[56]\ttrain's binary_logloss: 0.159754\tvalid's binary_logloss: 0.231636\n",
      "[57]\ttrain's binary_logloss: 0.157388\tvalid's binary_logloss: 0.230193\n",
      "[58]\ttrain's binary_logloss: 0.155472\tvalid's binary_logloss: 0.228464\n",
      "[59]\ttrain's binary_logloss: 0.153031\tvalid's binary_logloss: 0.226847\n",
      "[60]\ttrain's binary_logloss: 0.151532\tvalid's binary_logloss: 0.226568\n",
      "[61]\ttrain's binary_logloss: 0.15017\tvalid's binary_logloss: 0.226009\n",
      "[62]\ttrain's binary_logloss: 0.148635\tvalid's binary_logloss: 0.224963\n",
      "[63]\ttrain's binary_logloss: 0.146553\tvalid's binary_logloss: 0.224136\n",
      "[64]\ttrain's binary_logloss: 0.145045\tvalid's binary_logloss: 0.223451\n",
      "[65]\ttrain's binary_logloss: 0.143727\tvalid's binary_logloss: 0.222807\n",
      "[66]\ttrain's binary_logloss: 0.142241\tvalid's binary_logloss: 0.221845\n",
      "[67]\ttrain's binary_logloss: 0.140315\tvalid's binary_logloss: 0.220445\n",
      "[68]\ttrain's binary_logloss: 0.138894\tvalid's binary_logloss: 0.219881\n",
      "[69]\ttrain's binary_logloss: 0.137552\tvalid's binary_logloss: 0.219157\n",
      "[70]\ttrain's binary_logloss: 0.136268\tvalid's binary_logloss: 0.218572\n",
      "[71]\ttrain's binary_logloss: 0.134878\tvalid's binary_logloss: 0.218023\n",
      "[72]\ttrain's binary_logloss: 0.133489\tvalid's binary_logloss: 0.217522\n",
      "[73]\ttrain's binary_logloss: 0.132288\tvalid's binary_logloss: 0.216799\n",
      "[74]\ttrain's binary_logloss: 0.13093\tvalid's binary_logloss: 0.216275\n",
      "[75]\ttrain's binary_logloss: 0.129458\tvalid's binary_logloss: 0.215622\n",
      "[76]\ttrain's binary_logloss: 0.128267\tvalid's binary_logloss: 0.215018\n",
      "[77]\ttrain's binary_logloss: 0.127159\tvalid's binary_logloss: 0.214359\n",
      "[78]\ttrain's binary_logloss: 0.125919\tvalid's binary_logloss: 0.214125\n",
      "[79]\ttrain's binary_logloss: 0.124661\tvalid's binary_logloss: 0.213362\n",
      "[80]\ttrain's binary_logloss: 0.123555\tvalid's binary_logloss: 0.212817\n",
      "[81]\ttrain's binary_logloss: 0.122301\tvalid's binary_logloss: 0.212443\n",
      "[82]\ttrain's binary_logloss: 0.121177\tvalid's binary_logloss: 0.212212\n",
      "[83]\ttrain's binary_logloss: 0.120084\tvalid's binary_logloss: 0.21142\n",
      "[84]\ttrain's binary_logloss: 0.118992\tvalid's binary_logloss: 0.211128\n",
      "[85]\ttrain's binary_logloss: 0.117695\tvalid's binary_logloss: 0.210816\n",
      "[86]\ttrain's binary_logloss: 0.116487\tvalid's binary_logloss: 0.210593\n",
      "[87]\ttrain's binary_logloss: 0.115484\tvalid's binary_logloss: 0.210566\n",
      "[88]\ttrain's binary_logloss: 0.114101\tvalid's binary_logloss: 0.209503\n",
      "[89]\ttrain's binary_logloss: 0.112966\tvalid's binary_logloss: 0.209238\n",
      "[90]\ttrain's binary_logloss: 0.111862\tvalid's binary_logloss: 0.208966\n",
      "[91]\ttrain's binary_logloss: 0.110951\tvalid's binary_logloss: 0.208842\n",
      "[92]\ttrain's binary_logloss: 0.110017\tvalid's binary_logloss: 0.208357\n",
      "[93]\ttrain's binary_logloss: 0.109058\tvalid's binary_logloss: 0.208417\n",
      "[94]\ttrain's binary_logloss: 0.108028\tvalid's binary_logloss: 0.208343\n",
      "[95]\ttrain's binary_logloss: 0.106969\tvalid's binary_logloss: 0.207906\n",
      "[96]\ttrain's binary_logloss: 0.106108\tvalid's binary_logloss: 0.207697\n",
      "[97]\ttrain's binary_logloss: 0.105098\tvalid's binary_logloss: 0.207368\n",
      "[98]\ttrain's binary_logloss: 0.103745\tvalid's binary_logloss: 0.2068\n",
      "[99]\ttrain's binary_logloss: 0.102843\tvalid's binary_logloss: 0.206963\n",
      "[100]\ttrain's binary_logloss: 0.101981\tvalid's binary_logloss: 0.206682\n",
      "[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n",
      "This may cause significantly different results comparing to the previous versions of LightGBM.\n",
      "Try to set boost_from_average=false, if your old models produce bad results\n",
      "[1]\ttrain's binary_logloss: 0.454002\tvalid's binary_logloss: 0.463717\n",
      "[2]\ttrain's binary_logloss: 0.428562\tvalid's binary_logloss: 0.440947\n",
      "[3]\ttrain's binary_logloss: 0.407599\tvalid's binary_logloss: 0.422614\n",
      "[4]\ttrain's binary_logloss: 0.390282\tvalid's binary_logloss: 0.407262\n",
      "[5]\ttrain's binary_logloss: 0.375377\tvalid's binary_logloss: 0.395027\n",
      "[6]\ttrain's binary_logloss: 0.362372\tvalid's binary_logloss: 0.383689\n",
      "[7]\ttrain's binary_logloss: 0.35164\tvalid's binary_logloss: 0.37506\n",
      "[8]\ttrain's binary_logloss: 0.340887\tvalid's binary_logloss: 0.365699\n",
      "[9]\ttrain's binary_logloss: 0.330988\tvalid's binary_logloss: 0.357268\n",
      "[10]\ttrain's binary_logloss: 0.322747\tvalid's binary_logloss: 0.351206\n",
      "[11]\ttrain's binary_logloss: 0.314685\tvalid's binary_logloss: 0.343512\n",
      "[12]\ttrain's binary_logloss: 0.307382\tvalid's binary_logloss: 0.337971\n",
      "[13]\ttrain's binary_logloss: 0.299469\tvalid's binary_logloss: 0.331879\n",
      "[14]\ttrain's binary_logloss: 0.293242\tvalid's binary_logloss: 0.326949\n",
      "[15]\ttrain's binary_logloss: 0.287616\tvalid's binary_logloss: 0.322713\n",
      "[16]\ttrain's binary_logloss: 0.281928\tvalid's binary_logloss: 0.319091\n",
      "[17]\ttrain's binary_logloss: 0.275859\tvalid's binary_logloss: 0.314519\n",
      "[18]\ttrain's binary_logloss: 0.270841\tvalid's binary_logloss: 0.311057\n",
      "[19]\ttrain's binary_logloss: 0.266028\tvalid's binary_logloss: 0.30741\n",
      "[20]\ttrain's binary_logloss: 0.259543\tvalid's binary_logloss: 0.303093\n",
      "[21]\ttrain's binary_logloss: 0.254906\tvalid's binary_logloss: 0.300208\n",
      "[22]\ttrain's binary_logloss: 0.249399\tvalid's binary_logloss: 0.29631\n",
      "[23]\ttrain's binary_logloss: 0.244523\tvalid's binary_logloss: 0.292901\n",
      "[24]\ttrain's binary_logloss: 0.239795\tvalid's binary_logloss: 0.289645\n",
      "[25]\ttrain's binary_logloss: 0.236157\tvalid's binary_logloss: 0.286903\n",
      "[26]\ttrain's binary_logloss: 0.23234\tvalid's binary_logloss: 0.285204\n",
      "[27]\ttrain's binary_logloss: 0.227961\tvalid's binary_logloss: 0.282611\n",
      "[28]\ttrain's binary_logloss: 0.224817\tvalid's binary_logloss: 0.280282\n",
      "[29]\ttrain's binary_logloss: 0.220883\tvalid's binary_logloss: 0.278424\n",
      "[30]\ttrain's binary_logloss: 0.217562\tvalid's binary_logloss: 0.276957\n",
      "[31]\ttrain's binary_logloss: 0.214455\tvalid's binary_logloss: 0.27411\n",
      "[32]\ttrain's binary_logloss: 0.211108\tvalid's binary_logloss: 0.272863\n",
      "[33]\ttrain's binary_logloss: 0.208011\tvalid's binary_logloss: 0.271114\n",
      "[34]\ttrain's binary_logloss: 0.205489\tvalid's binary_logloss: 0.26993\n",
      "[35]\ttrain's binary_logloss: 0.202929\tvalid's binary_logloss: 0.268422\n",
      "[36]\ttrain's binary_logloss: 0.199286\tvalid's binary_logloss: 0.266072\n",
      "[37]\ttrain's binary_logloss: 0.196455\tvalid's binary_logloss: 0.264678\n",
      "[38]\ttrain's binary_logloss: 0.193871\tvalid's binary_logloss: 0.26313\n",
      "[39]\ttrain's binary_logloss: 0.191135\tvalid's binary_logloss: 0.261711\n",
      "[40]\ttrain's binary_logloss: 0.188875\tvalid's binary_logloss: 0.259951\n",
      "[41]\ttrain's binary_logloss: 0.186568\tvalid's binary_logloss: 0.259021\n",
      "[42]\ttrain's binary_logloss: 0.184058\tvalid's binary_logloss: 0.257889\n",
      "[43]\ttrain's binary_logloss: 0.181375\tvalid's binary_logloss: 0.256299\n",
      "[44]\ttrain's binary_logloss: 0.179181\tvalid's binary_logloss: 0.25533\n",
      "[45]\ttrain's binary_logloss: 0.17657\tvalid's binary_logloss: 0.254815\n",
      "[46]\ttrain's binary_logloss: 0.174711\tvalid's binary_logloss: 0.253911\n",
      "[47]\ttrain's binary_logloss: 0.172531\tvalid's binary_logloss: 0.253273\n",
      "[48]\ttrain's binary_logloss: 0.170531\tvalid's binary_logloss: 0.251909\n",
      "[49]\ttrain's binary_logloss: 0.168491\tvalid's binary_logloss: 0.250985\n",
      "[50]\ttrain's binary_logloss: 0.165977\tvalid's binary_logloss: 0.249564\n",
      "[51]\ttrain's binary_logloss: 0.164234\tvalid's binary_logloss: 0.249239\n",
      "[52]\ttrain's binary_logloss: 0.161815\tvalid's binary_logloss: 0.247996\n",
      "[53]\ttrain's binary_logloss: 0.160238\tvalid's binary_logloss: 0.247119\n",
      "[54]\ttrain's binary_logloss: 0.158453\tvalid's binary_logloss: 0.246054\n",
      "[55]\ttrain's binary_logloss: 0.156827\tvalid's binary_logloss: 0.245713\n",
      "[56]\ttrain's binary_logloss: 0.154813\tvalid's binary_logloss: 0.244461\n",
      "[57]\ttrain's binary_logloss: 0.15308\tvalid's binary_logloss: 0.243816\n",
      "[58]\ttrain's binary_logloss: 0.151192\tvalid's binary_logloss: 0.243238\n",
      "[59]\ttrain's binary_logloss: 0.149379\tvalid's binary_logloss: 0.242223\n",
      "[60]\ttrain's binary_logloss: 0.147302\tvalid's binary_logloss: 0.241406\n",
      "[61]\ttrain's binary_logloss: 0.145758\tvalid's binary_logloss: 0.241085\n",
      "[62]\ttrain's binary_logloss: 0.144182\tvalid's binary_logloss: 0.240885\n",
      "[63]\ttrain's binary_logloss: 0.14289\tvalid's binary_logloss: 0.240444\n",
      "[64]\ttrain's binary_logloss: 0.141317\tvalid's binary_logloss: 0.239756\n",
      "[65]\ttrain's binary_logloss: 0.139554\tvalid's binary_logloss: 0.238521\n",
      "[66]\ttrain's binary_logloss: 0.13809\tvalid's binary_logloss: 0.237868\n",
      "[67]\ttrain's binary_logloss: 0.136666\tvalid's binary_logloss: 0.237348\n",
      "[68]\ttrain's binary_logloss: 0.135253\tvalid's binary_logloss: 0.237173\n",
      "[69]\ttrain's binary_logloss: 0.133852\tvalid's binary_logloss: 0.23631\n",
      "[70]\ttrain's binary_logloss: 0.132582\tvalid's binary_logloss: 0.23586\n",
      "[71]\ttrain's binary_logloss: 0.131338\tvalid's binary_logloss: 0.235464\n",
      "[72]\ttrain's binary_logloss: 0.129755\tvalid's binary_logloss: 0.234707\n",
      "[73]\ttrain's binary_logloss: 0.128287\tvalid's binary_logloss: 0.233781\n",
      "[74]\ttrain's binary_logloss: 0.12702\tvalid's binary_logloss: 0.233736\n",
      "[75]\ttrain's binary_logloss: 0.125821\tvalid's binary_logloss: 0.233247\n",
      "[76]\ttrain's binary_logloss: 0.124549\tvalid's binary_logloss: 0.232647\n",
      "[77]\ttrain's binary_logloss: 0.123014\tvalid's binary_logloss: 0.231895\n",
      "[78]\ttrain's binary_logloss: 0.121895\tvalid's binary_logloss: 0.232004\n",
      "[79]\ttrain's binary_logloss: 0.12075\tvalid's binary_logloss: 0.231333\n",
      "[80]\ttrain's binary_logloss: 0.119603\tvalid's binary_logloss: 0.230905\n",
      "[81]\ttrain's binary_logloss: 0.118411\tvalid's binary_logloss: 0.230839\n",
      "[82]\ttrain's binary_logloss: 0.11718\tvalid's binary_logloss: 0.230068\n",
      "[83]\ttrain's binary_logloss: 0.116038\tvalid's binary_logloss: 0.22993\n",
      "[84]\ttrain's binary_logloss: 0.11454\tvalid's binary_logloss: 0.229423\n",
      "[85]\ttrain's binary_logloss: 0.113205\tvalid's binary_logloss: 0.228402\n",
      "[86]\ttrain's binary_logloss: 0.112145\tvalid's binary_logloss: 0.228431\n",
      "[87]\ttrain's binary_logloss: 0.111142\tvalid's binary_logloss: 0.228003\n",
      "[88]\ttrain's binary_logloss: 0.110108\tvalid's binary_logloss: 0.227613\n",
      "[89]\ttrain's binary_logloss: 0.109058\tvalid's binary_logloss: 0.227482\n",
      "[90]\ttrain's binary_logloss: 0.108082\tvalid's binary_logloss: 0.227313\n",
      "[91]\ttrain's binary_logloss: 0.106887\tvalid's binary_logloss: 0.226931\n",
      "[92]\ttrain's binary_logloss: 0.106037\tvalid's binary_logloss: 0.226633\n",
      "[93]\ttrain's binary_logloss: 0.105101\tvalid's binary_logloss: 0.226467\n",
      "[94]\ttrain's binary_logloss: 0.104193\tvalid's binary_logloss: 0.226735\n",
      "[95]\ttrain's binary_logloss: 0.103346\tvalid's binary_logloss: 0.226747\n",
      "[96]\ttrain's binary_logloss: 0.102144\tvalid's binary_logloss: 0.226233\n",
      "[97]\ttrain's binary_logloss: 0.101185\tvalid's binary_logloss: 0.225848\n",
      "[98]\ttrain's binary_logloss: 0.100395\tvalid's binary_logloss: 0.225934\n",
      "[99]\ttrain's binary_logloss: 0.0993901\tvalid's binary_logloss: 0.225952\n",
      "[100]\ttrain's binary_logloss: 0.0983998\tvalid's binary_logloss: 0.225351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21597230558462338"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "\n",
    "scores = []\n",
    "for tr_idx, va_idx, in kf.split(train_x):\n",
    "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "\n",
    "    model = Model()\n",
    "    model.fit(tr_x, tr_y, va_x, va_y)\n",
    "    va_pred = model.perdict(va_x)\n",
    "    score = log_loss(va_y, va_pred)\n",
    "    scores.append(score)\n",
    "\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500,)\n",
      "(2500,)\n",
      "(7500,)\n",
      "(2500,)\n",
      "(7500,)\n",
      "(2500,)\n",
      "(7500,)\n",
      "(2500,)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# Stratified K-Fold\n",
    "# -----------------------------------\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf = StratifiedKFold(n_splits=4, shuffle=True, random_state=71)\n",
    "\n",
    "for tr_idx, va_idx in kf.split(train_x, train_y):\n",
    "    print(tr_idx.shape)\n",
    "    print(va_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupKfold\n",
    "train_x[\"user_id\"] = np.arange(0, len(train_x)) // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          1\n",
       "        ... \n",
       "9995    2498\n",
       "9996    2499\n",
       "9997    2499\n",
       "9998    2499\n",
       "9999    2499\n",
       "Name: user_id, Length: 10000, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[\"user_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import  KFold, GroupKFold\n",
    "user_id = train_x['user_id']\n",
    "unique_user_ids = user_id.unique()\n",
    "unique_user_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1875,) (625,)\n",
      "(7500, 29) (2500, 29)\n",
      "(1875,) (625,)\n",
      "(7500, 29) (2500, 29)\n",
      "(1875,) (625,)\n",
      "(7500, 29) (2500, 29)\n",
      "(1875,) (625,)\n",
      "(7500, 29) (2500, 29)\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "for tr_group_idx, va_group_idx in kf.split(unique_user_ids):\n",
    "    tr_groups, va_groups = unique_user_ids[tr_group_idx], unique_user_ids[va_group_idx]\n",
    "\n",
    "    print(tr_groups.shape, va_groups.shape)\n",
    "\n",
    "    is_tr = user_id.isin(tr_groups)\n",
    "    is_va = user_id.isin(va_groups)\n",
    "\n",
    "    tr_x,va_x = train_x[is_tr], train_x[is_va]\n",
    "    print(tr_x.shape, va_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
