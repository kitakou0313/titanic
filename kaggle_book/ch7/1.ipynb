{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>product</th>\n",
       "      <th>amount</th>\n",
       "      <th>medical_info_a1</th>\n",
       "      <th>medical_info_a2</th>\n",
       "      <th>medical_info_a3</th>\n",
       "      <th>medical_info_b1</th>\n",
       "      <th>...</th>\n",
       "      <th>medical_keyword_5</th>\n",
       "      <th>medical_keyword_6</th>\n",
       "      <th>medical_keyword_7</th>\n",
       "      <th>medical_keyword_8</th>\n",
       "      <th>medical_keyword_9</th>\n",
       "      <th>medical_keyword_10</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>yearmonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>187.431987</td>\n",
       "      <td>81.008363</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "      <td>302</td>\n",
       "      <td>212</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>24204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>171.632630</td>\n",
       "      <td>71.067812</td>\n",
       "      <td>6</td>\n",
       "      <td>2000</td>\n",
       "      <td>197</td>\n",
       "      <td>469</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>24201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>163.543983</td>\n",
       "      <td>64.032098</td>\n",
       "      <td>0</td>\n",
       "      <td>4000000</td>\n",
       "      <td>247</td>\n",
       "      <td>225</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>24184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>150.391858</td>\n",
       "      <td>52.322910</td>\n",
       "      <td>2</td>\n",
       "      <td>1000000</td>\n",
       "      <td>108</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>24196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>165.835167</td>\n",
       "      <td>67.008154</td>\n",
       "      <td>2</td>\n",
       "      <td>4000000</td>\n",
       "      <td>181</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>24181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>185.174944</td>\n",
       "      <td>62.893499</td>\n",
       "      <td>8</td>\n",
       "      <td>3000</td>\n",
       "      <td>277</td>\n",
       "      <td>173</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>24183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>157.581442</td>\n",
       "      <td>58.889901</td>\n",
       "      <td>8</td>\n",
       "      <td>2000</td>\n",
       "      <td>184</td>\n",
       "      <td>206</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>24195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>177.676066</td>\n",
       "      <td>85.277018</td>\n",
       "      <td>2</td>\n",
       "      <td>6000000</td>\n",
       "      <td>443</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>24195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>166.757782</td>\n",
       "      <td>64.254215</td>\n",
       "      <td>3</td>\n",
       "      <td>6000000</td>\n",
       "      <td>267</td>\n",
       "      <td>193</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>24186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>158.656634</td>\n",
       "      <td>53.299956</td>\n",
       "      <td>1</td>\n",
       "      <td>3000000</td>\n",
       "      <td>317</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>24200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex      height     weight  product   amount  medical_info_a1  \\\n",
       "0      49    1  187.431987  81.008363        1  1000000              302   \n",
       "1      79    1  171.632630  71.067812        6     2000              197   \n",
       "2      78    0  163.543983  64.032098        0  4000000              247   \n",
       "3      26    1  150.391858  52.322910        2  1000000              108   \n",
       "4      14    1  165.835167  67.008154        2  4000000              181   \n",
       "...   ...  ...         ...        ...      ...      ...              ...   \n",
       "9995   21    1  185.174944  62.893499        8     3000              277   \n",
       "9996   34    1  157.581442  58.889901        8     2000              184   \n",
       "9997   36    1  177.676066  85.277018        2  6000000              443   \n",
       "9998   18    1  166.757782  64.254215        3  6000000              267   \n",
       "9999   42    0  158.656634  53.299956        1  3000000              317   \n",
       "\n",
       "      medical_info_a2  medical_info_a3  medical_info_b1  ...  \\\n",
       "0                 212                1               10  ...   \n",
       "1                 469                0               14  ...   \n",
       "2                 225                2               17  ...   \n",
       "3                 228                0               15  ...   \n",
       "4                  90                2               11  ...   \n",
       "...               ...              ...              ...  ...   \n",
       "9995              173                2               11  ...   \n",
       "9996              206                3               18  ...   \n",
       "9997              191                1               11  ...   \n",
       "9998              193                0               19  ...   \n",
       "9999              179                0               15  ...   \n",
       "\n",
       "      medical_keyword_5  medical_keyword_6  medical_keyword_7  \\\n",
       "0                     0                  1                  0   \n",
       "1                     0                  0                  0   \n",
       "2                     0                  1                  0   \n",
       "3                     0                  0                  1   \n",
       "4                     0                  0                  0   \n",
       "...                 ...                ...                ...   \n",
       "9995                  0                  0                  0   \n",
       "9996                  0                  1                  0   \n",
       "9997                  0                  0                  0   \n",
       "9998                  0                  0                  0   \n",
       "9999                  1                  1                  0   \n",
       "\n",
       "      medical_keyword_8  medical_keyword_9  medical_keyword_10  year  month  \\\n",
       "0                     1                  0                   0  2016     12   \n",
       "1                     0                  1                   1  2016      9   \n",
       "2                     1                  0                   0  2015      4   \n",
       "3                     0                  0                   0  2016      4   \n",
       "4                     1                  0                   0  2015      1   \n",
       "...                 ...                ...                 ...   ...    ...   \n",
       "9995                  1                  0                   0  2015      3   \n",
       "9996                  1                  0                   0  2016      3   \n",
       "9997                  1                  1                   0  2016      3   \n",
       "9998                  1                  0                   0  2015      6   \n",
       "9999                  1                  0                   0  2016      8   \n",
       "\n",
       "      day  yearmonth  \n",
       "0       6      24204  \n",
       "1       3      24201  \n",
       "2      10      24184  \n",
       "3      17      24196  \n",
       "4      26      24181  \n",
       "...   ...        ...  \n",
       "9995   11      24183  \n",
       "9996   27      24195  \n",
       "9997   16      24195  \n",
       "9998   17      24186  \n",
       "9999   15      24200  \n",
       "\n",
       "[10000 rows x 28 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/train_preprocessed.csv\")\n",
    "train_x = train.drop([\"target\"], axis=1)\n",
    "train_y = train[\"target\"]\n",
    "\n",
    "test_x = pd.read_csv(\"../data/test_preprocessed.csv\")\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのスタッキング\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from models import Model2Linear, ModleGBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cv(model:Union[ModleGBDT, Model2Linear], train_x:pd.DataFrame, train_y:pd.DataFrame, test_x:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    学習データに対する目的変数を知らない学習、予測を行い、テストデータに対する予測値を返す関数\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    preds_test = []\n",
    "    va_idxes = []\n",
    "\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "\n",
    "    for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n",
    "        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "\n",
    "        model.fit(tr_x, tr_y, va_x, va_y)\n",
    "\n",
    "        pred = model.predict(va_x)\n",
    "        preds.append(pred)\n",
    "\n",
    "        pred_test = model.predict(test_x)\n",
    "        preds_test.append(pred_test)\n",
    "\n",
    "        va_idxes.append(va_idx)\n",
    "\n",
    "        print(preds_test)\n",
    "    \n",
    "    # validationに対する予測値を連結、元の順序に並べなおす\n",
    "    va_idxes = np.concatenate(va_idxes)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "    order = np.argsort(va_idxes)\n",
    "    pred_train = preds[order]\n",
    "\n",
    "    preds_test = np.mean(preds_test, axis=0)\n",
    "\n",
    "    return pred_train, preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n",
      "This may cause significantly different results comparing to the previous versions of LightGBM.\n",
      "Try to set boost_from_average=false, if your old models produce bad results\n",
      "[1]\ttrain's binary_logloss: 0.454308\tvalid's binary_logloss: 0.465515\n",
      "[2]\ttrain's binary_logloss: 0.429565\tvalid's binary_logloss: 0.443444\n",
      "[3]\ttrain's binary_logloss: 0.410077\tvalid's binary_logloss: 0.425543\n",
      "[4]\ttrain's binary_logloss: 0.39358\tvalid's binary_logloss: 0.410625\n",
      "[5]\ttrain's binary_logloss: 0.379354\tvalid's binary_logloss: 0.397666\n",
      "[6]\ttrain's binary_logloss: 0.365913\tvalid's binary_logloss: 0.387422\n",
      "[7]\ttrain's binary_logloss: 0.354309\tvalid's binary_logloss: 0.376037\n",
      "[8]\ttrain's binary_logloss: 0.344354\tvalid's binary_logloss: 0.366734\n",
      "[9]\ttrain's binary_logloss: 0.334834\tvalid's binary_logloss: 0.35898\n",
      "[10]\ttrain's binary_logloss: 0.326209\tvalid's binary_logloss: 0.351612\n",
      "[11]\ttrain's binary_logloss: 0.317809\tvalid's binary_logloss: 0.34563\n",
      "[12]\ttrain's binary_logloss: 0.310845\tvalid's binary_logloss: 0.340564\n",
      "[13]\ttrain's binary_logloss: 0.30401\tvalid's binary_logloss: 0.334274\n",
      "[14]\ttrain's binary_logloss: 0.296333\tvalid's binary_logloss: 0.327911\n",
      "[15]\ttrain's binary_logloss: 0.290137\tvalid's binary_logloss: 0.324239\n",
      "[16]\ttrain's binary_logloss: 0.283293\tvalid's binary_logloss: 0.317865\n",
      "[17]\ttrain's binary_logloss: 0.277823\tvalid's binary_logloss: 0.314412\n",
      "[18]\ttrain's binary_logloss: 0.272322\tvalid's binary_logloss: 0.310773\n",
      "[19]\ttrain's binary_logloss: 0.26705\tvalid's binary_logloss: 0.307374\n",
      "[20]\ttrain's binary_logloss: 0.262091\tvalid's binary_logloss: 0.303946\n",
      "[21]\ttrain's binary_logloss: 0.257394\tvalid's binary_logloss: 0.301154\n",
      "[22]\ttrain's binary_logloss: 0.253115\tvalid's binary_logloss: 0.298085\n",
      "[23]\ttrain's binary_logloss: 0.24878\tvalid's binary_logloss: 0.294533\n",
      "[24]\ttrain's binary_logloss: 0.243951\tvalid's binary_logloss: 0.291832\n",
      "[25]\ttrain's binary_logloss: 0.240469\tvalid's binary_logloss: 0.289538\n",
      "[26]\ttrain's binary_logloss: 0.236038\tvalid's binary_logloss: 0.285777\n",
      "[27]\ttrain's binary_logloss: 0.231149\tvalid's binary_logloss: 0.281554\n",
      "[28]\ttrain's binary_logloss: 0.227936\tvalid's binary_logloss: 0.279469\n",
      "[29]\ttrain's binary_logloss: 0.224501\tvalid's binary_logloss: 0.278255\n",
      "[30]\ttrain's binary_logloss: 0.221476\tvalid's binary_logloss: 0.27687\n",
      "[31]\ttrain's binary_logloss: 0.218174\tvalid's binary_logloss: 0.274434\n",
      "[32]\ttrain's binary_logloss: 0.214462\tvalid's binary_logloss: 0.272274\n",
      "[33]\ttrain's binary_logloss: 0.21088\tvalid's binary_logloss: 0.269122\n",
      "[34]\ttrain's binary_logloss: 0.207684\tvalid's binary_logloss: 0.266756\n",
      "[35]\ttrain's binary_logloss: 0.205168\tvalid's binary_logloss: 0.265291\n",
      "[36]\ttrain's binary_logloss: 0.202792\tvalid's binary_logloss: 0.264635\n",
      "[37]\ttrain's binary_logloss: 0.200275\tvalid's binary_logloss: 0.263032\n",
      "[38]\ttrain's binary_logloss: 0.19745\tvalid's binary_logloss: 0.261202\n",
      "[39]\ttrain's binary_logloss: 0.194507\tvalid's binary_logloss: 0.25904\n",
      "[40]\ttrain's binary_logloss: 0.192425\tvalid's binary_logloss: 0.257587\n",
      "[41]\ttrain's binary_logloss: 0.190121\tvalid's binary_logloss: 0.256767\n",
      "[42]\ttrain's binary_logloss: 0.187834\tvalid's binary_logloss: 0.255625\n",
      "[43]\ttrain's binary_logloss: 0.184909\tvalid's binary_logloss: 0.254088\n",
      "[44]\ttrain's binary_logloss: 0.181938\tvalid's binary_logloss: 0.252355\n",
      "[45]\ttrain's binary_logloss: 0.180014\tvalid's binary_logloss: 0.251427\n",
      "[46]\ttrain's binary_logloss: 0.177952\tvalid's binary_logloss: 0.250039\n",
      "[47]\ttrain's binary_logloss: 0.175486\tvalid's binary_logloss: 0.248749\n",
      "[48]\ttrain's binary_logloss: 0.17332\tvalid's binary_logloss: 0.247716\n",
      "[49]\ttrain's binary_logloss: 0.171233\tvalid's binary_logloss: 0.247249\n",
      "[50]\ttrain's binary_logloss: 0.168808\tvalid's binary_logloss: 0.245712\n",
      "[51]\ttrain's binary_logloss: 0.166901\tvalid's binary_logloss: 0.244631\n",
      "[52]\ttrain's binary_logloss: 0.164701\tvalid's binary_logloss: 0.243459\n",
      "[53]\ttrain's binary_logloss: 0.162987\tvalid's binary_logloss: 0.242513\n",
      "[54]\ttrain's binary_logloss: 0.161101\tvalid's binary_logloss: 0.24179\n",
      "[55]\ttrain's binary_logloss: 0.159227\tvalid's binary_logloss: 0.240958\n",
      "[56]\ttrain's binary_logloss: 0.157179\tvalid's binary_logloss: 0.23982\n",
      "[57]\ttrain's binary_logloss: 0.155645\tvalid's binary_logloss: 0.239127\n",
      "[58]\ttrain's binary_logloss: 0.153637\tvalid's binary_logloss: 0.237746\n",
      "[59]\ttrain's binary_logloss: 0.152156\tvalid's binary_logloss: 0.237403\n",
      "[60]\ttrain's binary_logloss: 0.150492\tvalid's binary_logloss: 0.236752\n",
      "[61]\ttrain's binary_logloss: 0.14833\tvalid's binary_logloss: 0.235299\n",
      "[62]\ttrain's binary_logloss: 0.146708\tvalid's binary_logloss: 0.234711\n",
      "[63]\ttrain's binary_logloss: 0.145146\tvalid's binary_logloss: 0.234231\n",
      "[64]\ttrain's binary_logloss: 0.143475\tvalid's binary_logloss: 0.233571\n",
      "[65]\ttrain's binary_logloss: 0.141857\tvalid's binary_logloss: 0.233159\n",
      "[66]\ttrain's binary_logloss: 0.140617\tvalid's binary_logloss: 0.232806\n",
      "[67]\ttrain's binary_logloss: 0.139349\tvalid's binary_logloss: 0.232361\n",
      "[68]\ttrain's binary_logloss: 0.137656\tvalid's binary_logloss: 0.231805\n",
      "[69]\ttrain's binary_logloss: 0.13622\tvalid's binary_logloss: 0.231595\n",
      "[70]\ttrain's binary_logloss: 0.134909\tvalid's binary_logloss: 0.231123\n",
      "[71]\ttrain's binary_logloss: 0.133365\tvalid's binary_logloss: 0.23016\n",
      "[72]\ttrain's binary_logloss: 0.131928\tvalid's binary_logloss: 0.22954\n",
      "[73]\ttrain's binary_logloss: 0.130743\tvalid's binary_logloss: 0.2294\n",
      "[74]\ttrain's binary_logloss: 0.129119\tvalid's binary_logloss: 0.228424\n",
      "[75]\ttrain's binary_logloss: 0.127482\tvalid's binary_logloss: 0.227763\n",
      "[76]\ttrain's binary_logloss: 0.126147\tvalid's binary_logloss: 0.227717\n",
      "[77]\ttrain's binary_logloss: 0.124614\tvalid's binary_logloss: 0.226361\n",
      "[78]\ttrain's binary_logloss: 0.123468\tvalid's binary_logloss: 0.226123\n",
      "[79]\ttrain's binary_logloss: 0.121894\tvalid's binary_logloss: 0.224743\n",
      "[80]\ttrain's binary_logloss: 0.120732\tvalid's binary_logloss: 0.224614\n",
      "[81]\ttrain's binary_logloss: 0.119582\tvalid's binary_logloss: 0.224197\n",
      "[82]\ttrain's binary_logloss: 0.118468\tvalid's binary_logloss: 0.223638\n",
      "[83]\ttrain's binary_logloss: 0.117333\tvalid's binary_logloss: 0.223198\n",
      "[84]\ttrain's binary_logloss: 0.116301\tvalid's binary_logloss: 0.223096\n",
      "[85]\ttrain's binary_logloss: 0.11519\tvalid's binary_logloss: 0.222564\n",
      "[86]\ttrain's binary_logloss: 0.114083\tvalid's binary_logloss: 0.222174\n",
      "[87]\ttrain's binary_logloss: 0.112979\tvalid's binary_logloss: 0.222011\n",
      "[88]\ttrain's binary_logloss: 0.11165\tvalid's binary_logloss: 0.220694\n",
      "[89]\ttrain's binary_logloss: 0.110383\tvalid's binary_logloss: 0.220025\n",
      "[90]\ttrain's binary_logloss: 0.109327\tvalid's binary_logloss: 0.219546\n",
      "[91]\ttrain's binary_logloss: 0.108284\tvalid's binary_logloss: 0.218836\n",
      "[92]\ttrain's binary_logloss: 0.107136\tvalid's binary_logloss: 0.21845\n",
      "[93]\ttrain's binary_logloss: 0.106131\tvalid's binary_logloss: 0.218435\n",
      "[94]\ttrain's binary_logloss: 0.105149\tvalid's binary_logloss: 0.218246\n",
      "[95]\ttrain's binary_logloss: 0.104193\tvalid's binary_logloss: 0.218185\n",
      "[96]\ttrain's binary_logloss: 0.103162\tvalid's binary_logloss: 0.217907\n",
      "[97]\ttrain's binary_logloss: 0.102197\tvalid's binary_logloss: 0.217536\n",
      "[98]\ttrain's binary_logloss: 0.101254\tvalid's binary_logloss: 0.217335\n",
      "[99]\ttrain's binary_logloss: 0.100266\tvalid's binary_logloss: 0.217277\n",
      "[100]\ttrain's binary_logloss: 0.0994527\tvalid's binary_logloss: 0.217264\n",
      "[array([0.13039508, 0.07698264, 0.01107451, ..., 0.79174978, 0.0022922 ,\n",
      "       0.37540876])]\n",
      "[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n",
      "This may cause significantly different results comparing to the previous versions of LightGBM.\n",
      "Try to set boost_from_average=false, if your old models produce bad results\n",
      "[1]\ttrain's binary_logloss: 0.460683\tvalid's binary_logloss: 0.445527\n",
      "[2]\ttrain's binary_logloss: 0.434914\tvalid's binary_logloss: 0.42475\n",
      "[3]\ttrain's binary_logloss: 0.414154\tvalid's binary_logloss: 0.408448\n",
      "[4]\ttrain's binary_logloss: 0.396952\tvalid's binary_logloss: 0.394103\n",
      "[5]\ttrain's binary_logloss: 0.38195\tvalid's binary_logloss: 0.382367\n",
      "[6]\ttrain's binary_logloss: 0.369146\tvalid's binary_logloss: 0.372506\n",
      "[7]\ttrain's binary_logloss: 0.357112\tvalid's binary_logloss: 0.364028\n",
      "[8]\ttrain's binary_logloss: 0.346977\tvalid's binary_logloss: 0.357331\n",
      "[9]\ttrain's binary_logloss: 0.337622\tvalid's binary_logloss: 0.350197\n",
      "[10]\ttrain's binary_logloss: 0.329478\tvalid's binary_logloss: 0.344383\n",
      "[11]\ttrain's binary_logloss: 0.321132\tvalid's binary_logloss: 0.338466\n",
      "[12]\ttrain's binary_logloss: 0.313617\tvalid's binary_logloss: 0.332766\n",
      "[13]\ttrain's binary_logloss: 0.30608\tvalid's binary_logloss: 0.326439\n",
      "[14]\ttrain's binary_logloss: 0.299119\tvalid's binary_logloss: 0.321705\n",
      "[15]\ttrain's binary_logloss: 0.292493\tvalid's binary_logloss: 0.316989\n",
      "[16]\ttrain's binary_logloss: 0.285813\tvalid's binary_logloss: 0.311977\n",
      "[17]\ttrain's binary_logloss: 0.27985\tvalid's binary_logloss: 0.308307\n",
      "[18]\ttrain's binary_logloss: 0.274492\tvalid's binary_logloss: 0.305441\n",
      "[19]\ttrain's binary_logloss: 0.269809\tvalid's binary_logloss: 0.301833\n",
      "[20]\ttrain's binary_logloss: 0.264112\tvalid's binary_logloss: 0.298228\n",
      "[21]\ttrain's binary_logloss: 0.259142\tvalid's binary_logloss: 0.294624\n",
      "[22]\ttrain's binary_logloss: 0.254988\tvalid's binary_logloss: 0.291626\n",
      "[23]\ttrain's binary_logloss: 0.2509\tvalid's binary_logloss: 0.289124\n",
      "[24]\ttrain's binary_logloss: 0.246887\tvalid's binary_logloss: 0.28657\n",
      "[25]\ttrain's binary_logloss: 0.24273\tvalid's binary_logloss: 0.284375\n",
      "[26]\ttrain's binary_logloss: 0.238828\tvalid's binary_logloss: 0.282064\n",
      "[27]\ttrain's binary_logloss: 0.234758\tvalid's binary_logloss: 0.279011\n",
      "[28]\ttrain's binary_logloss: 0.231367\tvalid's binary_logloss: 0.27719\n",
      "[29]\ttrain's binary_logloss: 0.226885\tvalid's binary_logloss: 0.27392\n",
      "[30]\ttrain's binary_logloss: 0.223622\tvalid's binary_logloss: 0.271819\n",
      "[31]\ttrain's binary_logloss: 0.220823\tvalid's binary_logloss: 0.26999\n",
      "[32]\ttrain's binary_logloss: 0.218027\tvalid's binary_logloss: 0.268667\n",
      "[33]\ttrain's binary_logloss: 0.214645\tvalid's binary_logloss: 0.266851\n",
      "[34]\ttrain's binary_logloss: 0.21106\tvalid's binary_logloss: 0.263664\n",
      "[35]\ttrain's binary_logloss: 0.208429\tvalid's binary_logloss: 0.262366\n",
      "[36]\ttrain's binary_logloss: 0.205435\tvalid's binary_logloss: 0.260092\n",
      "[37]\ttrain's binary_logloss: 0.202718\tvalid's binary_logloss: 0.258452\n",
      "[38]\ttrain's binary_logloss: 0.199477\tvalid's binary_logloss: 0.255889\n",
      "[39]\ttrain's binary_logloss: 0.196729\tvalid's binary_logloss: 0.254161\n",
      "[40]\ttrain's binary_logloss: 0.193862\tvalid's binary_logloss: 0.252688\n",
      "[41]\ttrain's binary_logloss: 0.191261\tvalid's binary_logloss: 0.250924\n",
      "[42]\ttrain's binary_logloss: 0.188936\tvalid's binary_logloss: 0.250589\n",
      "[43]\ttrain's binary_logloss: 0.186726\tvalid's binary_logloss: 0.249751\n",
      "[44]\ttrain's binary_logloss: 0.184547\tvalid's binary_logloss: 0.248667\n",
      "[45]\ttrain's binary_logloss: 0.182229\tvalid's binary_logloss: 0.247304\n",
      "[46]\ttrain's binary_logloss: 0.180278\tvalid's binary_logloss: 0.24663\n",
      "[47]\ttrain's binary_logloss: 0.178227\tvalid's binary_logloss: 0.245724\n",
      "[48]\ttrain's binary_logloss: 0.17598\tvalid's binary_logloss: 0.244177\n",
      "[49]\ttrain's binary_logloss: 0.174074\tvalid's binary_logloss: 0.243811\n",
      "[50]\ttrain's binary_logloss: 0.171338\tvalid's binary_logloss: 0.24192\n",
      "[51]\ttrain's binary_logloss: 0.169476\tvalid's binary_logloss: 0.241339\n",
      "[52]\ttrain's binary_logloss: 0.167169\tvalid's binary_logloss: 0.239601\n",
      "[53]\ttrain's binary_logloss: 0.165316\tvalid's binary_logloss: 0.238659\n",
      "[54]\ttrain's binary_logloss: 0.163015\tvalid's binary_logloss: 0.236957\n",
      "[55]\ttrain's binary_logloss: 0.160341\tvalid's binary_logloss: 0.234712\n",
      "[56]\ttrain's binary_logloss: 0.158558\tvalid's binary_logloss: 0.234069\n",
      "[57]\ttrain's binary_logloss: 0.156989\tvalid's binary_logloss: 0.23333\n",
      "[58]\ttrain's binary_logloss: 0.155397\tvalid's binary_logloss: 0.2327\n",
      "[59]\ttrain's binary_logloss: 0.153571\tvalid's binary_logloss: 0.231853\n",
      "[60]\ttrain's binary_logloss: 0.151882\tvalid's binary_logloss: 0.231119\n",
      "[61]\ttrain's binary_logloss: 0.150035\tvalid's binary_logloss: 0.229544\n",
      "[62]\ttrain's binary_logloss: 0.148515\tvalid's binary_logloss: 0.229169\n",
      "[63]\ttrain's binary_logloss: 0.146908\tvalid's binary_logloss: 0.228923\n",
      "[64]\ttrain's binary_logloss: 0.145537\tvalid's binary_logloss: 0.228752\n",
      "[65]\ttrain's binary_logloss: 0.144203\tvalid's binary_logloss: 0.228824\n",
      "[66]\ttrain's binary_logloss: 0.1428\tvalid's binary_logloss: 0.228232\n",
      "[67]\ttrain's binary_logloss: 0.141316\tvalid's binary_logloss: 0.227693\n",
      "[68]\ttrain's binary_logloss: 0.139916\tvalid's binary_logloss: 0.226982\n",
      "[69]\ttrain's binary_logloss: 0.138496\tvalid's binary_logloss: 0.226912\n",
      "[70]\ttrain's binary_logloss: 0.136849\tvalid's binary_logloss: 0.225778\n",
      "[71]\ttrain's binary_logloss: 0.135715\tvalid's binary_logloss: 0.225521\n",
      "[72]\ttrain's binary_logloss: 0.13433\tvalid's binary_logloss: 0.224983\n",
      "[73]\ttrain's binary_logloss: 0.132912\tvalid's binary_logloss: 0.224385\n",
      "[74]\ttrain's binary_logloss: 0.13138\tvalid's binary_logloss: 0.223778\n",
      "[75]\ttrain's binary_logloss: 0.130124\tvalid's binary_logloss: 0.222861\n",
      "[76]\ttrain's binary_logloss: 0.128931\tvalid's binary_logloss: 0.222522\n",
      "[77]\ttrain's binary_logloss: 0.127775\tvalid's binary_logloss: 0.222622\n",
      "[78]\ttrain's binary_logloss: 0.126455\tvalid's binary_logloss: 0.222063\n",
      "[79]\ttrain's binary_logloss: 0.124851\tvalid's binary_logloss: 0.221238\n",
      "[80]\ttrain's binary_logloss: 0.123553\tvalid's binary_logloss: 0.220734\n",
      "[81]\ttrain's binary_logloss: 0.122352\tvalid's binary_logloss: 0.220331\n",
      "[82]\ttrain's binary_logloss: 0.121223\tvalid's binary_logloss: 0.219743\n",
      "[83]\ttrain's binary_logloss: 0.1202\tvalid's binary_logloss: 0.220056\n",
      "[84]\ttrain's binary_logloss: 0.119073\tvalid's binary_logloss: 0.219459\n",
      "[85]\ttrain's binary_logloss: 0.117989\tvalid's binary_logloss: 0.219275\n",
      "[86]\ttrain's binary_logloss: 0.116666\tvalid's binary_logloss: 0.218472\n",
      "[87]\ttrain's binary_logloss: 0.115579\tvalid's binary_logloss: 0.218394\n",
      "[88]\ttrain's binary_logloss: 0.114462\tvalid's binary_logloss: 0.217985\n",
      "[89]\ttrain's binary_logloss: 0.113412\tvalid's binary_logloss: 0.217221\n",
      "[90]\ttrain's binary_logloss: 0.112221\tvalid's binary_logloss: 0.217117\n",
      "[91]\ttrain's binary_logloss: 0.111297\tvalid's binary_logloss: 0.2171\n",
      "[92]\ttrain's binary_logloss: 0.110277\tvalid's binary_logloss: 0.216819\n",
      "[93]\ttrain's binary_logloss: 0.109143\tvalid's binary_logloss: 0.216398\n",
      "[94]\ttrain's binary_logloss: 0.107932\tvalid's binary_logloss: 0.215407\n",
      "[95]\ttrain's binary_logloss: 0.106959\tvalid's binary_logloss: 0.215293\n",
      "[96]\ttrain's binary_logloss: 0.106024\tvalid's binary_logloss: 0.214868\n",
      "[97]\ttrain's binary_logloss: 0.105188\tvalid's binary_logloss: 0.214679\n",
      "[98]\ttrain's binary_logloss: 0.104204\tvalid's binary_logloss: 0.214062\n",
      "[99]\ttrain's binary_logloss: 0.103322\tvalid's binary_logloss: 0.214188\n",
      "[100]\ttrain's binary_logloss: 0.102367\tvalid's binary_logloss: 0.214592\n",
      "[array([0.13039508, 0.07698264, 0.01107451, ..., 0.79174978, 0.0022922 ,\n",
      "       0.37540876]), array([0.26651587, 0.16752662, 0.01580849, ..., 0.78095458, 0.00187877,\n",
      "       0.5396211 ])]\n",
      "[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n",
      "This may cause significantly different results comparing to the previous versions of LightGBM.\n",
      "Try to set boost_from_average=false, if your old models produce bad results\n",
      "[1]\ttrain's binary_logloss: 0.454514\tvalid's binary_logloss: 0.462302\n",
      "[2]\ttrain's binary_logloss: 0.430155\tvalid's binary_logloss: 0.439446\n",
      "[3]\ttrain's binary_logloss: 0.411274\tvalid's binary_logloss: 0.420192\n",
      "[4]\ttrain's binary_logloss: 0.395891\tvalid's binary_logloss: 0.405234\n",
      "[5]\ttrain's binary_logloss: 0.380893\tvalid's binary_logloss: 0.391113\n",
      "[6]\ttrain's binary_logloss: 0.3687\tvalid's binary_logloss: 0.381125\n",
      "[7]\ttrain's binary_logloss: 0.357678\tvalid's binary_logloss: 0.371047\n",
      "[8]\ttrain's binary_logloss: 0.347273\tvalid's binary_logloss: 0.362708\n",
      "[9]\ttrain's binary_logloss: 0.3384\tvalid's binary_logloss: 0.355273\n",
      "[10]\ttrain's binary_logloss: 0.329459\tvalid's binary_logloss: 0.346679\n",
      "[11]\ttrain's binary_logloss: 0.321347\tvalid's binary_logloss: 0.340116\n",
      "[12]\ttrain's binary_logloss: 0.314839\tvalid's binary_logloss: 0.334649\n",
      "[13]\ttrain's binary_logloss: 0.308432\tvalid's binary_logloss: 0.328457\n",
      "[14]\ttrain's binary_logloss: 0.302163\tvalid's binary_logloss: 0.323778\n",
      "[15]\ttrain's binary_logloss: 0.295853\tvalid's binary_logloss: 0.318936\n",
      "[16]\ttrain's binary_logloss: 0.289292\tvalid's binary_logloss: 0.314108\n",
      "[17]\ttrain's binary_logloss: 0.28355\tvalid's binary_logloss: 0.309519\n",
      "[18]\ttrain's binary_logloss: 0.278249\tvalid's binary_logloss: 0.305123\n",
      "[19]\ttrain's binary_logloss: 0.27248\tvalid's binary_logloss: 0.299955\n",
      "[20]\ttrain's binary_logloss: 0.267242\tvalid's binary_logloss: 0.295732\n",
      "[21]\ttrain's binary_logloss: 0.262732\tvalid's binary_logloss: 0.292204\n",
      "[22]\ttrain's binary_logloss: 0.258047\tvalid's binary_logloss: 0.289008\n",
      "[23]\ttrain's binary_logloss: 0.252391\tvalid's binary_logloss: 0.284907\n",
      "[24]\ttrain's binary_logloss: 0.247533\tvalid's binary_logloss: 0.281473\n",
      "[25]\ttrain's binary_logloss: 0.243537\tvalid's binary_logloss: 0.278999\n",
      "[26]\ttrain's binary_logloss: 0.238803\tvalid's binary_logloss: 0.275479\n",
      "[27]\ttrain's binary_logloss: 0.235009\tvalid's binary_logloss: 0.27302\n",
      "[28]\ttrain's binary_logloss: 0.230733\tvalid's binary_logloss: 0.27064\n",
      "[29]\ttrain's binary_logloss: 0.226691\tvalid's binary_logloss: 0.267774\n",
      "[30]\ttrain's binary_logloss: 0.222935\tvalid's binary_logloss: 0.265414\n",
      "[31]\ttrain's binary_logloss: 0.220249\tvalid's binary_logloss: 0.264213\n",
      "[32]\ttrain's binary_logloss: 0.217233\tvalid's binary_logloss: 0.262515\n",
      "[33]\ttrain's binary_logloss: 0.213647\tvalid's binary_logloss: 0.260202\n",
      "[34]\ttrain's binary_logloss: 0.210791\tvalid's binary_logloss: 0.259205\n",
      "[35]\ttrain's binary_logloss: 0.208228\tvalid's binary_logloss: 0.258056\n",
      "[36]\ttrain's binary_logloss: 0.205292\tvalid's binary_logloss: 0.256281\n",
      "[37]\ttrain's binary_logloss: 0.201464\tvalid's binary_logloss: 0.253669\n",
      "[38]\ttrain's binary_logloss: 0.199071\tvalid's binary_logloss: 0.2523\n",
      "[39]\ttrain's binary_logloss: 0.196644\tvalid's binary_logloss: 0.250643\n",
      "[40]\ttrain's binary_logloss: 0.193564\tvalid's binary_logloss: 0.249192\n",
      "[41]\ttrain's binary_logloss: 0.191324\tvalid's binary_logloss: 0.247782\n",
      "[42]\ttrain's binary_logloss: 0.188767\tvalid's binary_logloss: 0.246162\n",
      "[43]\ttrain's binary_logloss: 0.186339\tvalid's binary_logloss: 0.244493\n",
      "[44]\ttrain's binary_logloss: 0.183478\tvalid's binary_logloss: 0.243403\n",
      "[45]\ttrain's binary_logloss: 0.181354\tvalid's binary_logloss: 0.242426\n",
      "[46]\ttrain's binary_logloss: 0.179377\tvalid's binary_logloss: 0.241998\n",
      "[47]\ttrain's binary_logloss: 0.177363\tvalid's binary_logloss: 0.240868\n",
      "[48]\ttrain's binary_logloss: 0.175493\tvalid's binary_logloss: 0.240088\n",
      "[49]\ttrain's binary_logloss: 0.173663\tvalid's binary_logloss: 0.238919\n",
      "[50]\ttrain's binary_logloss: 0.170515\tvalid's binary_logloss: 0.236455\n",
      "[51]\ttrain's binary_logloss: 0.168623\tvalid's binary_logloss: 0.235594\n",
      "[52]\ttrain's binary_logloss: 0.166865\tvalid's binary_logloss: 0.234834\n",
      "[53]\ttrain's binary_logloss: 0.165164\tvalid's binary_logloss: 0.234259\n",
      "[54]\ttrain's binary_logloss: 0.163417\tvalid's binary_logloss: 0.233238\n",
      "[55]\ttrain's binary_logloss: 0.161513\tvalid's binary_logloss: 0.232266\n",
      "[56]\ttrain's binary_logloss: 0.159754\tvalid's binary_logloss: 0.231636\n",
      "[57]\ttrain's binary_logloss: 0.157388\tvalid's binary_logloss: 0.230193\n",
      "[58]\ttrain's binary_logloss: 0.155472\tvalid's binary_logloss: 0.228464\n",
      "[59]\ttrain's binary_logloss: 0.153031\tvalid's binary_logloss: 0.226847\n",
      "[60]\ttrain's binary_logloss: 0.151532\tvalid's binary_logloss: 0.226568\n",
      "[61]\ttrain's binary_logloss: 0.15017\tvalid's binary_logloss: 0.226009\n",
      "[62]\ttrain's binary_logloss: 0.148635\tvalid's binary_logloss: 0.224963\n",
      "[63]\ttrain's binary_logloss: 0.146553\tvalid's binary_logloss: 0.224136\n",
      "[64]\ttrain's binary_logloss: 0.145045\tvalid's binary_logloss: 0.223451\n",
      "[65]\ttrain's binary_logloss: 0.143727\tvalid's binary_logloss: 0.222807\n",
      "[66]\ttrain's binary_logloss: 0.142241\tvalid's binary_logloss: 0.221845\n",
      "[67]\ttrain's binary_logloss: 0.140315\tvalid's binary_logloss: 0.220445\n",
      "[68]\ttrain's binary_logloss: 0.138894\tvalid's binary_logloss: 0.219881\n",
      "[69]\ttrain's binary_logloss: 0.137552\tvalid's binary_logloss: 0.219157\n",
      "[70]\ttrain's binary_logloss: 0.136268\tvalid's binary_logloss: 0.218572\n",
      "[71]\ttrain's binary_logloss: 0.134878\tvalid's binary_logloss: 0.218023\n",
      "[72]\ttrain's binary_logloss: 0.133489\tvalid's binary_logloss: 0.217522\n",
      "[73]\ttrain's binary_logloss: 0.132288\tvalid's binary_logloss: 0.216799\n",
      "[74]\ttrain's binary_logloss: 0.13093\tvalid's binary_logloss: 0.216275\n",
      "[75]\ttrain's binary_logloss: 0.129458\tvalid's binary_logloss: 0.215622\n",
      "[76]\ttrain's binary_logloss: 0.128267\tvalid's binary_logloss: 0.215018\n",
      "[77]\ttrain's binary_logloss: 0.127159\tvalid's binary_logloss: 0.214359\n",
      "[78]\ttrain's binary_logloss: 0.125919\tvalid's binary_logloss: 0.214125\n",
      "[79]\ttrain's binary_logloss: 0.124661\tvalid's binary_logloss: 0.213362\n",
      "[80]\ttrain's binary_logloss: 0.123555\tvalid's binary_logloss: 0.212817\n",
      "[81]\ttrain's binary_logloss: 0.122301\tvalid's binary_logloss: 0.212443\n",
      "[82]\ttrain's binary_logloss: 0.121177\tvalid's binary_logloss: 0.212212\n",
      "[83]\ttrain's binary_logloss: 0.120084\tvalid's binary_logloss: 0.21142\n",
      "[84]\ttrain's binary_logloss: 0.118992\tvalid's binary_logloss: 0.211128\n",
      "[85]\ttrain's binary_logloss: 0.117695\tvalid's binary_logloss: 0.210816\n",
      "[86]\ttrain's binary_logloss: 0.116487\tvalid's binary_logloss: 0.210593\n",
      "[87]\ttrain's binary_logloss: 0.115484\tvalid's binary_logloss: 0.210566\n",
      "[88]\ttrain's binary_logloss: 0.114101\tvalid's binary_logloss: 0.209503\n",
      "[89]\ttrain's binary_logloss: 0.112966\tvalid's binary_logloss: 0.209238\n",
      "[90]\ttrain's binary_logloss: 0.111862\tvalid's binary_logloss: 0.208966\n",
      "[91]\ttrain's binary_logloss: 0.110951\tvalid's binary_logloss: 0.208842\n",
      "[92]\ttrain's binary_logloss: 0.110017\tvalid's binary_logloss: 0.208357\n",
      "[93]\ttrain's binary_logloss: 0.109058\tvalid's binary_logloss: 0.208417\n",
      "[94]\ttrain's binary_logloss: 0.108028\tvalid's binary_logloss: 0.208343\n",
      "[95]\ttrain's binary_logloss: 0.106969\tvalid's binary_logloss: 0.207906\n",
      "[96]\ttrain's binary_logloss: 0.106108\tvalid's binary_logloss: 0.207697\n",
      "[97]\ttrain's binary_logloss: 0.105098\tvalid's binary_logloss: 0.207368\n",
      "[98]\ttrain's binary_logloss: 0.103745\tvalid's binary_logloss: 0.2068\n",
      "[99]\ttrain's binary_logloss: 0.102843\tvalid's binary_logloss: 0.206963\n",
      "[100]\ttrain's binary_logloss: 0.101981\tvalid's binary_logloss: 0.206682\n",
      "[array([0.13039508, 0.07698264, 0.01107451, ..., 0.79174978, 0.0022922 ,\n",
      "       0.37540876]), array([0.26651587, 0.16752662, 0.01580849, ..., 0.78095458, 0.00187877,\n",
      "       0.5396211 ]), array([0.22231421, 0.11758783, 0.0193913 , ..., 0.86415103, 0.00218983,\n",
      "       0.39016105])]\n",
      "[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n",
      "This may cause significantly different results comparing to the previous versions of LightGBM.\n",
      "Try to set boost_from_average=false, if your old models produce bad results\n",
      "[1]\ttrain's binary_logloss: 0.454002\tvalid's binary_logloss: 0.463717\n",
      "[2]\ttrain's binary_logloss: 0.428562\tvalid's binary_logloss: 0.440947\n",
      "[3]\ttrain's binary_logloss: 0.407599\tvalid's binary_logloss: 0.422614\n",
      "[4]\ttrain's binary_logloss: 0.390282\tvalid's binary_logloss: 0.407262\n",
      "[5]\ttrain's binary_logloss: 0.375377\tvalid's binary_logloss: 0.395027\n",
      "[6]\ttrain's binary_logloss: 0.362372\tvalid's binary_logloss: 0.383689\n",
      "[7]\ttrain's binary_logloss: 0.35164\tvalid's binary_logloss: 0.37506\n",
      "[8]\ttrain's binary_logloss: 0.340887\tvalid's binary_logloss: 0.365699\n",
      "[9]\ttrain's binary_logloss: 0.330988\tvalid's binary_logloss: 0.357268\n",
      "[10]\ttrain's binary_logloss: 0.322747\tvalid's binary_logloss: 0.351206\n",
      "[11]\ttrain's binary_logloss: 0.314685\tvalid's binary_logloss: 0.343512\n",
      "[12]\ttrain's binary_logloss: 0.307382\tvalid's binary_logloss: 0.337971\n",
      "[13]\ttrain's binary_logloss: 0.299469\tvalid's binary_logloss: 0.331879\n",
      "[14]\ttrain's binary_logloss: 0.293242\tvalid's binary_logloss: 0.326949\n",
      "[15]\ttrain's binary_logloss: 0.287616\tvalid's binary_logloss: 0.322713\n",
      "[16]\ttrain's binary_logloss: 0.281928\tvalid's binary_logloss: 0.319091\n",
      "[17]\ttrain's binary_logloss: 0.275859\tvalid's binary_logloss: 0.314519\n",
      "[18]\ttrain's binary_logloss: 0.270841\tvalid's binary_logloss: 0.311057\n",
      "[19]\ttrain's binary_logloss: 0.266028\tvalid's binary_logloss: 0.30741\n",
      "[20]\ttrain's binary_logloss: 0.259543\tvalid's binary_logloss: 0.303093\n",
      "[21]\ttrain's binary_logloss: 0.254906\tvalid's binary_logloss: 0.300208\n",
      "[22]\ttrain's binary_logloss: 0.249399\tvalid's binary_logloss: 0.29631\n",
      "[23]\ttrain's binary_logloss: 0.244523\tvalid's binary_logloss: 0.292901\n",
      "[24]\ttrain's binary_logloss: 0.239795\tvalid's binary_logloss: 0.289645\n",
      "[25]\ttrain's binary_logloss: 0.236157\tvalid's binary_logloss: 0.286903\n",
      "[26]\ttrain's binary_logloss: 0.23234\tvalid's binary_logloss: 0.285204\n",
      "[27]\ttrain's binary_logloss: 0.227961\tvalid's binary_logloss: 0.282611\n",
      "[28]\ttrain's binary_logloss: 0.224817\tvalid's binary_logloss: 0.280282\n",
      "[29]\ttrain's binary_logloss: 0.220883\tvalid's binary_logloss: 0.278424\n",
      "[30]\ttrain's binary_logloss: 0.217562\tvalid's binary_logloss: 0.276957\n",
      "[31]\ttrain's binary_logloss: 0.214455\tvalid's binary_logloss: 0.27411\n",
      "[32]\ttrain's binary_logloss: 0.211108\tvalid's binary_logloss: 0.272863\n",
      "[33]\ttrain's binary_logloss: 0.208011\tvalid's binary_logloss: 0.271114\n",
      "[34]\ttrain's binary_logloss: 0.205489\tvalid's binary_logloss: 0.26993\n",
      "[35]\ttrain's binary_logloss: 0.202929\tvalid's binary_logloss: 0.268422\n",
      "[36]\ttrain's binary_logloss: 0.199286\tvalid's binary_logloss: 0.266072\n",
      "[37]\ttrain's binary_logloss: 0.196455\tvalid's binary_logloss: 0.264678\n",
      "[38]\ttrain's binary_logloss: 0.193871\tvalid's binary_logloss: 0.26313\n",
      "[39]\ttrain's binary_logloss: 0.191135\tvalid's binary_logloss: 0.261711\n",
      "[40]\ttrain's binary_logloss: 0.188875\tvalid's binary_logloss: 0.259951\n",
      "[41]\ttrain's binary_logloss: 0.186568\tvalid's binary_logloss: 0.259021\n",
      "[42]\ttrain's binary_logloss: 0.184058\tvalid's binary_logloss: 0.257889\n",
      "[43]\ttrain's binary_logloss: 0.181375\tvalid's binary_logloss: 0.256299\n",
      "[44]\ttrain's binary_logloss: 0.179181\tvalid's binary_logloss: 0.25533\n",
      "[45]\ttrain's binary_logloss: 0.17657\tvalid's binary_logloss: 0.254815\n",
      "[46]\ttrain's binary_logloss: 0.174711\tvalid's binary_logloss: 0.253911\n",
      "[47]\ttrain's binary_logloss: 0.172531\tvalid's binary_logloss: 0.253273\n",
      "[48]\ttrain's binary_logloss: 0.170531\tvalid's binary_logloss: 0.251909\n",
      "[49]\ttrain's binary_logloss: 0.168491\tvalid's binary_logloss: 0.250985\n",
      "[50]\ttrain's binary_logloss: 0.165977\tvalid's binary_logloss: 0.249564\n",
      "[51]\ttrain's binary_logloss: 0.164234\tvalid's binary_logloss: 0.249239\n",
      "[52]\ttrain's binary_logloss: 0.161815\tvalid's binary_logloss: 0.247996\n",
      "[53]\ttrain's binary_logloss: 0.160238\tvalid's binary_logloss: 0.247119\n",
      "[54]\ttrain's binary_logloss: 0.158453\tvalid's binary_logloss: 0.246054\n",
      "[55]\ttrain's binary_logloss: 0.156827\tvalid's binary_logloss: 0.245713\n",
      "[56]\ttrain's binary_logloss: 0.154813\tvalid's binary_logloss: 0.244461\n",
      "[57]\ttrain's binary_logloss: 0.15308\tvalid's binary_logloss: 0.243816\n",
      "[58]\ttrain's binary_logloss: 0.151192\tvalid's binary_logloss: 0.243238\n",
      "[59]\ttrain's binary_logloss: 0.149379\tvalid's binary_logloss: 0.242223\n",
      "[60]\ttrain's binary_logloss: 0.147302\tvalid's binary_logloss: 0.241406\n",
      "[61]\ttrain's binary_logloss: 0.145758\tvalid's binary_logloss: 0.241085\n",
      "[62]\ttrain's binary_logloss: 0.144182\tvalid's binary_logloss: 0.240885\n",
      "[63]\ttrain's binary_logloss: 0.14289\tvalid's binary_logloss: 0.240444\n",
      "[64]\ttrain's binary_logloss: 0.141317\tvalid's binary_logloss: 0.239756\n",
      "[65]\ttrain's binary_logloss: 0.139554\tvalid's binary_logloss: 0.238521\n",
      "[66]\ttrain's binary_logloss: 0.13809\tvalid's binary_logloss: 0.237868\n",
      "[67]\ttrain's binary_logloss: 0.136666\tvalid's binary_logloss: 0.237348\n",
      "[68]\ttrain's binary_logloss: 0.135253\tvalid's binary_logloss: 0.237173\n",
      "[69]\ttrain's binary_logloss: 0.133852\tvalid's binary_logloss: 0.23631\n",
      "[70]\ttrain's binary_logloss: 0.132582\tvalid's binary_logloss: 0.23586\n",
      "[71]\ttrain's binary_logloss: 0.131338\tvalid's binary_logloss: 0.235464\n",
      "[72]\ttrain's binary_logloss: 0.129755\tvalid's binary_logloss: 0.234707\n",
      "[73]\ttrain's binary_logloss: 0.128287\tvalid's binary_logloss: 0.233781\n",
      "[74]\ttrain's binary_logloss: 0.12702\tvalid's binary_logloss: 0.233736\n",
      "[75]\ttrain's binary_logloss: 0.125821\tvalid's binary_logloss: 0.233247\n",
      "[76]\ttrain's binary_logloss: 0.124549\tvalid's binary_logloss: 0.232647\n",
      "[77]\ttrain's binary_logloss: 0.123014\tvalid's binary_logloss: 0.231895\n",
      "[78]\ttrain's binary_logloss: 0.121895\tvalid's binary_logloss: 0.232004\n",
      "[79]\ttrain's binary_logloss: 0.12075\tvalid's binary_logloss: 0.231333\n",
      "[80]\ttrain's binary_logloss: 0.119603\tvalid's binary_logloss: 0.230905\n",
      "[81]\ttrain's binary_logloss: 0.118411\tvalid's binary_logloss: 0.230839\n",
      "[82]\ttrain's binary_logloss: 0.11718\tvalid's binary_logloss: 0.230068\n",
      "[83]\ttrain's binary_logloss: 0.116038\tvalid's binary_logloss: 0.22993\n",
      "[84]\ttrain's binary_logloss: 0.11454\tvalid's binary_logloss: 0.229423\n",
      "[85]\ttrain's binary_logloss: 0.113205\tvalid's binary_logloss: 0.228402\n",
      "[86]\ttrain's binary_logloss: 0.112145\tvalid's binary_logloss: 0.228431\n",
      "[87]\ttrain's binary_logloss: 0.111142\tvalid's binary_logloss: 0.228003\n",
      "[88]\ttrain's binary_logloss: 0.110108\tvalid's binary_logloss: 0.227613\n",
      "[89]\ttrain's binary_logloss: 0.109058\tvalid's binary_logloss: 0.227482\n",
      "[90]\ttrain's binary_logloss: 0.108082\tvalid's binary_logloss: 0.227313\n",
      "[91]\ttrain's binary_logloss: 0.106887\tvalid's binary_logloss: 0.226931\n",
      "[92]\ttrain's binary_logloss: 0.106037\tvalid's binary_logloss: 0.226633\n",
      "[93]\ttrain's binary_logloss: 0.105101\tvalid's binary_logloss: 0.226467\n",
      "[94]\ttrain's binary_logloss: 0.104193\tvalid's binary_logloss: 0.226735\n",
      "[95]\ttrain's binary_logloss: 0.103346\tvalid's binary_logloss: 0.226747\n",
      "[96]\ttrain's binary_logloss: 0.102144\tvalid's binary_logloss: 0.226233\n",
      "[97]\ttrain's binary_logloss: 0.101185\tvalid's binary_logloss: 0.225848\n",
      "[98]\ttrain's binary_logloss: 0.100395\tvalid's binary_logloss: 0.225934\n",
      "[99]\ttrain's binary_logloss: 0.0993901\tvalid's binary_logloss: 0.225952\n",
      "[100]\ttrain's binary_logloss: 0.0983998\tvalid's binary_logloss: 0.225351\n",
      "[array([0.13039508, 0.07698264, 0.01107451, ..., 0.79174978, 0.0022922 ,\n",
      "       0.37540876]), array([0.26651587, 0.16752662, 0.01580849, ..., 0.78095458, 0.00187877,\n",
      "       0.5396211 ]), array([0.22231421, 0.11758783, 0.0193913 , ..., 0.86415103, 0.00218983,\n",
      "       0.39016105]), array([0.13799529, 0.04073809, 0.0128464 , ..., 0.79269738, 0.00294236,\n",
      "       0.34247928])]\n",
      "logloss: 0.2160\n"
     ]
    }
   ],
   "source": [
    "model_1a = ModleGBDT()\n",
    "pred_train_1a, pred_test_1a = predict_cv(\n",
    "    model=model_1a, train_x=train_x, train_y=train_y, test_x=test_x\n",
    ")\n",
    "\n",
    "print(f'logloss: {log_loss(train_y, pred_train_1a, eps=1e-7):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0168521 , 0.08079163, 0.73945262, ..., 0.00476939, 0.00251913,\n",
       "       0.00287568])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train_1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_1a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.080792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.739453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.446095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.029752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.273593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.004769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.002519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.002876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred_1a\n",
       "0     0.016852\n",
       "1     0.080792\n",
       "2     0.739453\n",
       "3     0.002048\n",
       "4     0.446095\n",
       "...        ...\n",
       "9995  0.029752\n",
       "9996  0.273593\n",
       "9997  0.004769\n",
       "9998  0.002519\n",
       "9999  0.002876\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_2 = pd.DataFrame({\"pred_1a\":pred_train_1a})\n",
    "test_x_2 = pd.DataFrame({\"pred_1a\":pred_test_1a})\n",
    "\n",
    "train_x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.1099099 , 0.05753015, 0.02990078, ..., 0.94392942, 0.02715792,\n",
      "       0.42033222])]\n",
      "[array([0.1099099 , 0.05753015, 0.02990078, ..., 0.94392942, 0.02715792,\n",
      "       0.42033222]), array([0.11179359, 0.05818056, 0.03004526, ..., 0.94748128, 0.02726274,\n",
      "       0.42940997])]\n",
      "[array([0.1099099 , 0.05753015, 0.02990078, ..., 0.94392942, 0.02715792,\n",
      "       0.42033222]), array([0.11179359, 0.05818056, 0.03004526, ..., 0.94748128, 0.02726274,\n",
      "       0.42940997]), array([0.11131275, 0.05858234, 0.03060699, ..., 0.9428351 , 0.02782013,\n",
      "       0.4207574 ])]\n",
      "[array([0.1099099 , 0.05753015, 0.02990078, ..., 0.94392942, 0.02715792,\n",
      "       0.42033222]), array([0.11179359, 0.05818056, 0.03004526, ..., 0.94748128, 0.02726274,\n",
      "       0.42940997]), array([0.11131275, 0.05858234, 0.03060699, ..., 0.9428351 , 0.02782013,\n",
      "       0.4207574 ]), array([0.10544075, 0.05477073, 0.02827816, ..., 0.94351755, 0.02566066,\n",
      "       0.41242049])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'logloss: 0.2290'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = Model2Linear()\n",
    "\n",
    "pred_train_2, pred_test_2 = predict_cv(model_2, train_x_2, train_y, test_x_2)\n",
    "\n",
    "f'logloss: {log_loss(train_y, pred_train_2, eps=1e-7):.4f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
